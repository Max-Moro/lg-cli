"""
–§–∏–∫—Å—Ç—É—Ä—ã –∏ —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–æ–≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.

–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç:
- mock –¥–ª—è hf_hub_download (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥—Å–∫–∞—á–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ resources/)
- —Ñ–∏–∫—Å—Ç—É—Ä—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤
- —Ö–µ–ª–ø–µ—Ä—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–µ—à–µ–º –º–æ–¥–µ–ª–µ–π
"""

import json
import shutil
from pathlib import Path
from typing import Optional

import pytest


# ==================== –ü—É—Ç–∏ –∫ —Ä–µ—Å—É—Ä—Å–∞–º ====================

@pytest.fixture(scope="session")
def resources_dir() -> Path:
    """–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –ø—Ä–µ–¥—Å–∫–∞—á–∞–Ω–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –¥–ª—è —Ç–µ—Å—Ç–æ–≤."""
    return Path(__file__).parent / "resources"


@pytest.fixture(scope="session")
def models_manifest(resources_dir: Path) -> dict:
    """
    –ú–∞–Ω–∏—Ñ–µ—Å—Ç —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–µ–¥—Å–∫–∞—á–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö.
    
    –§–æ—Ä–º–∞—Ç:
    {
        "tokenizers": {
            "gpt2": {"filename": "tokenizer.json", "description": "..."},
            ...
        },
        "sentencepiece": {
            "google/gemma-2-2b": {"filename": "tokenizer.model", "description": "..."},
            ...
        }
    }
    """
    manifest_file = resources_dir / "models_manifest.json"
    if not manifest_file.exists():
        return {"tokenizers": {}, "sentencepiece": {}}
    
    with manifest_file.open("r", encoding="utf-8") as f:
        return json.load(f)


# ==================== Mock –¥–ª—è HuggingFace Hub ====================

class MockHFHub:
    """Mock –¥–ª—è hf_hub_download, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã."""
    
    def __init__(self, resources_dir: Path, manifest: dict):
        self.resources_dir = resources_dir
        self.manifest = manifest
        self.download_count = 0
    
    def download(
        self, 
        repo_id: str, 
        filename: str, 
        cache_dir: Optional[str] = None,
        local_dir: Optional[str] = None,
        local_dir_use_symlinks: bool = False,
        **kwargs
    ) -> str:
        """
        –ò–º–∏—Ç–∏—Ä—É–µ—Ç hf_hub_download, –Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ñ–∞–π–ª—ã –∏–∑ resources/.
        
        –ö–æ–ø–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ resources/ –≤ cache_dir –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø–æ–≤–µ–¥–µ–Ω–∏—è.
        """
        self.download_count += 1
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        if filename == "tokenizer.json":
            lib_type = "tokenizers"
        elif filename in ("tokenizer.model", "spiece.model", "sentencepiece.model"):
            lib_type = "sentencepiece"
        else:
            raise FileNotFoundError(f"Unknown model file type: {filename}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏ –≤ –º–∞–Ω–∏—Ñ–µ—Å—Ç–µ
        models = self.manifest.get(lib_type, {})
        if repo_id not in models:
            raise FileNotFoundError(
                f"Model '{repo_id}' not found in test resources. "
                f"Available models: {list(models.keys())}"
            )
        
        model_info = models[repo_id]
        source_file = self.resources_dir / lib_type / repo_id.replace("/", "--") / model_info["filename"]
        
        if not source_file.exists():
            raise FileNotFoundError(f"Model file not found in resources: {source_file}")
        
        # –ö–æ–ø–∏—Ä—É–µ–º –≤ cache_dir (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
        if local_dir:
            target_dir = Path(local_dir)
            target_dir.mkdir(parents=True, exist_ok=True)
            target_file = target_dir / filename
            shutil.copy2(source_file, target_file)
            return str(target_file)
        elif cache_dir:
            target_dir = Path(cache_dir)
            target_dir.mkdir(parents=True, exist_ok=True)
            target_file = target_dir / filename
            shutil.copy2(source_file, target_file)
            return str(target_file)
        else:
            # –ë–µ–∑ –∫–µ—à–∞ - –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –≤ resources
            return str(source_file)


@pytest.fixture
def mock_hf_hub(resources_dir: Path, models_manifest: dict, monkeypatch):
    """
    –ü–æ–¥–º–µ–Ω—è–µ—Ç hf_hub_download –Ω–∞ mock, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã.
    
    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ—Å—Ç–∞—Ö:
        def test_something(mock_hf_hub):
            # hf_hub_download —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
            adapter = HFAdapter("gpt2", tmp_path)
            ...
            
            # –ú–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Å–∫–∞—á–∏–≤–∞–ª–∞—Å—å –º–æ–¥–µ–ª—å
            assert mock_hf_hub.download_count == 1
    """
    mock = MockHFHub(resources_dir, models_manifest)
    
    # –ü–∞—Ç—á–∏–º hf_hub_download –≤ –æ–±–æ–∏—Ö –∞–¥–∞–ø—Ç–µ—Ä–∞—Ö
    monkeypatch.setattr(
        "lg.stats.tokenizers.hf_adapter.hf_hub_download",
        mock.download
    )
    monkeypatch.setattr(
        "lg.stats.tokenizers.sp_adapter.hf_hub_download", 
        mock.download
    )
    
    return mock


# ==================== –§–∏–∫—Å—Ç—É—Ä—ã –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ ====================

@pytest.fixture
def tiktoken_service(tmp_path: Path):
    """–°–æ–∑–¥–∞–µ—Ç TokenService —Å tiktoken —ç–Ω–∫–æ–¥–µ—Ä–æ–º (–¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏)."""
    from lg.stats.tokenizer import TokenService
    from tests.infrastructure.cli_utils import DEFAULT_TOKENIZER_LIB, DEFAULT_ENCODER
    
    return TokenService(
        root=tmp_path,
        lib=DEFAULT_TOKENIZER_LIB,
        encoder=DEFAULT_ENCODER
    )


@pytest.fixture
def hf_tokenizer_service(tmp_path: Path, mock_hf_hub):
    """–°–æ–∑–¥–∞–µ—Ç TokenService —Å HuggingFace —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º."""
    from lg.stats.tokenizer import TokenService
    
    return TokenService(
        root=tmp_path,
        lib="tokenizers",
        encoder="gpt2"  # –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–æ–≤
    )


@pytest.fixture
def sp_tokenizer_service(tmp_path: Path, mock_hf_hub):
    """–°–æ–∑–¥–∞–µ—Ç TokenService —Å SentencePiece —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º."""
    from lg.stats.tokenizer import TokenService
    
    return TokenService(
        root=tmp_path,
        lib="sentencepiece",
        encoder="google/gemma-2-2b"  # –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–æ–≤
    )


# ==================== –§–∏–∫—Å—Ç—É—Ä–∞ –¥–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ ====================

@pytest.fixture
def sample_texts() -> dict:
    """
    –ù–∞–±–æ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤.
    
    –í–∫–ª—é—á–∞–µ—Ç:
    - –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç
    - –ö–æ–¥ –Ω–∞ Python
    - –°–º–µ—à–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
    - –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
    """
    return {
        "simple": "Hello, world! This is a test.",
        "python_code": '''
def hello_world():
    """Prints hello world."""
    print("Hello, world!")
    return 42
'''.strip(),
        "mixed": '''
# Documentation

Here is some Python code:

```python
def process_data(items):
    return [x * 2 for x in items]
```

And some explanation text.
'''.strip(),
        "special_chars": "Special: √±, √º, ‰Ω†Â•Ω, emoji üöÄ, symbols @#$%",
        "long_text": " ".join(["This is a longer text with repeated words."] * 50),
    }


# ==================== –£—Ç–∏–ª–∏—Ç—ã ====================

@pytest.fixture
def clear_model_cache(tmp_path: Path):
    """–û—á–∏—â–∞–µ—Ç –∫–µ—à –º–æ–¥–µ–ª–µ–π –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º —Ç–µ—Å—Ç–æ–º."""
    cache_dir = tmp_path / ".lg-cache" / "tokenizer-models"
    if cache_dir.exists():
        shutil.rmtree(cache_dir)
    cache_dir.mkdir(parents=True, exist_ok=True)
    return cache_dir
