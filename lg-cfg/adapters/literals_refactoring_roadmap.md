# –î–æ—Ä–æ–∂–Ω–∞—è –∫–∞—Ä—Ç–∞ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ Literals Optimization

–≠—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–ª–∞–Ω –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è –∫–æ–¥–∞ –∫ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ, –æ–ø–∏—Å–∞–Ω–Ω–æ–π –≤ `literals_architecture.md`.

---

## ‚úÖ –í—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ —ç—Ç–∞–ø—ã (–≠—Ç–∞–ø—ã 1-4)

### –≠—Ç–∞–ø 1: –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã utils/ –∏ –ø–µ—Ä–µ–Ω–æ—Å —É—Ç–∏–ª–∏—Ç
**–°—Ç–∞—Ç—É—Å**: ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω (commit 304bbec)

**–†–µ–∑—É–ª—å—Ç–∞—Ç**:
- –°–æ–∑–¥–∞–Ω –ø–∞–∫–µ—Ç `utils/` –¥–ª—è —É—Ç–∏–ª–∏—Ç–∞—Ä–Ω—ã—Ö –º–æ–¥—É–ª–µ–π
- –ü–µ—Ä–µ–Ω–µ—Å–µ–Ω—ã: `element_parser.py`, `budgeting.py`, `interpolation.py`
- –û–±–Ω–æ–≤–ª–µ–Ω—ã –≤—Å–µ –∏–º–ø–æ—Ä—Ç—ã –≤ 7 –º–æ–¥—É–ª—è—Ö
- `components/` —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã

---

### –≠—Ç–∞–ø 2: –°–ª–∏—è–Ω–∏–µ PlaceholderCommentFormatter —Å ResultFormatter
**–°—Ç–∞—Ç—É—Å**: ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω (commit f067fe3)

**–†–µ–∑—É–ª—å—Ç–∞—Ç**:
- –£–¥–∞–ª–µ–Ω –ª–æ–∂–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç `components/placeholder.py`
- –õ–æ–≥–∏–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è placeholder –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –≤ `ResultFormatter`
- 5 –º–µ—Ç–æ–¥–æ–≤ –¥–æ–±–∞–≤–ª–µ–Ω—ã –∫–∞–∫ –ø—Ä–∏–≤–∞—Ç–Ω—ã–µ –≤ `ResultFormatter`
- –ß–∏—Å—Ç–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ: 122 insertions, 156 deletions

---

### –≠—Ç–∞–ø 3: –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ LiteralParser –º–µ—Ç–æ–¥–∞–º–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—Ç—Å—Ç—É–ø–æ–≤
**–°—Ç–∞—Ç—É—Å**: ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω (commit c078f04)

**–†–µ–∑—É–ª—å—Ç–∞—Ç**:
- –î–æ–±–∞–≤–ª–µ–Ω—ã —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã: `detect_base_indent()`, `detect_element_indent()`
- –î–æ–±–∞–≤–ª–µ–Ω –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π API: `parse_from_node()`
- –£–¥–∞–ª–µ–Ω—ã –º–µ—Ç–æ–¥—ã `_get_base_indent()` –∏ `_get_element_indent()` –∏–∑ pipeline
- Pipeline –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π API –ø–∞—Ä—Å–µ—Ä–∞
- –ß–∏—Å—Ç–æ–µ —É–ø—Ä–æ—â–µ–Ω–∏–µ: 153 insertions, 82 deletions

---

### –≠—Ç–∞–ø 4: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ can_handle() –≤ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
**–°—Ç–∞—Ç—É—Å**: ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω (commit fe59bd2)

**–†–µ–∑—É–ª—å—Ç–∞—Ç**:
- –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å—Ç–∞–ª–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–Ω–æ–º–Ω—ã–º–∏
- –î–æ–±–∞–≤–ª–µ–Ω—ã –º–µ—Ç–æ–¥—ã `can_handle()` –≤ `ASTSequenceProcessor` –∏ `BlockInitProcessor`
- –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∞–º–∏ –∏–∑–≤–ª–µ–∫–∞—é—Ç –¥–∞–Ω–Ω—ã–µ –∏ –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
- –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏–≥–Ω–∞—Ç—É—Ä–∞ `process(node, doc, source_text, profile, token_budget)`
- –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: 167 insertions, 46 deletions

---

## üöß –¢–µ–∫—É—â–∏–π —ç—Ç–∞–ø

### –≠—Ç–∞–ø 5: –£–ø—Ä–æ—â–µ–Ω–∏–µ pipeline –¥–æ —á–∏—Å—Ç–æ–≥–æ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞

**–¶–µ–ª—å**: –ü—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å pipeline –≤ —ç–ª–µ–≥–∞–Ω—Ç–Ω—ã–π –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä ~250 —Å—Ç—Ä–æ–∫ (—Å–µ–π—á–∞—Å ~700)

**–¢–µ–∫—É—â–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ pipeline (~700 —Å—Ç—Ä–æ–∫)**:
```
LiteralPipeline (processing/pipeline.py)
‚îú‚îÄ‚îÄ apply()                                 # Entry point
‚îú‚îÄ‚îÄ _process_strings()                      # Pass 1
‚îú‚îÄ‚îÄ _process_collections()                  # Pass 2
‚îú‚îÄ‚îÄ _process_profile()                      # Common routing
‚îú‚îÄ‚îÄ _process_block_init_node()             # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–æ—É—Ç–µ—Ä
‚îú‚îÄ‚îÄ _process_sequence_node()               # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–æ—É—Ç–µ—Ä
‚îú‚îÄ‚îÄ _process_standard_collection_node()    # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–æ—É—Ç–µ—Ä
‚îú‚îÄ‚îÄ _process_literal_impl()                # –ù–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
‚îú‚îÄ‚îÄ _process_string()                       # String processing
‚îú‚îÄ‚îÄ _process_collection_dfs()              # Collection processing
‚îú‚îÄ‚îÄ _apply_trim_result()                   # Apply results
‚îú‚îÄ‚îÄ _apply_trim_result_composing()         # Apply results (composing)
‚îî‚îÄ‚îÄ ... –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
```

**–ü—Ä–æ–±–ª–µ–º—ã —Ç–µ–∫—É—â–µ–≥–æ pipeline**:
1. **–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–æ—É—Ç–µ—Ä—ã** (`_process_*_node`) ‚Äî –¥—É–±–ª–∏—Ä—É—é—Ç –ª–æ–≥–∏–∫—É –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏
2. **–ù–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞** (`_process_literal_impl`) ‚Äî —Å–º–µ—à–∏–≤–∞–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—é –∏ –ª–æ–≥–∏–∫—É
3. **–£—Å–ª–æ–≤–∏—è –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç–∏** ‚Äî –≤ pipeline –≤–º–µ—Å—Ç–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
4. **–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤** ‚Äî pipeline –≥–æ—Ç–æ–≤–∏—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

**–¶–µ–ª–µ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ pipeline (~250 —Å—Ç—Ä–æ–∫)**:
```
LiteralPipeline (processing/pipeline.py)
‚îú‚îÄ‚îÄ apply()                          # Entry point (–¥–≤—É—Ö–ø—Ä–æ—Ö–æ–¥–Ω–∞—è –ª–æ–≥–∏–∫–∞)
‚îú‚îÄ‚îÄ _process_strings()               # Pass 1 coordinator
‚îú‚îÄ‚îÄ _process_collections()           # Pass 2 coordinator
‚îú‚îÄ‚îÄ _process_profile()               # Unified profile processing
‚îú‚îÄ‚îÄ _process_literal()               # ‚≠ê –ï–î–ò–ù–ê–Ø —Ç–æ—á–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
‚îÇ   ‚îú‚îÄ‚îÄ –ü—Ä–æ–≤–µ—Ä–∫–∞ can_handle() –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
‚îÇ   ‚îî‚îÄ‚îÄ –†–æ—É—Ç–∏–Ω–≥ –Ω–∞ _process_string() –∏–ª–∏ _process_collection()
‚îú‚îÄ‚îÄ _process_string()                # Simplified string processing
‚îú‚îÄ‚îÄ _process_collection()            # Simplified collection processing
‚îî‚îÄ‚îÄ _apply_result()                  # Unified result application
```

---

### –î–µ–π—Å—Ç–≤–∏—è –ø–æ –≠—Ç–∞–ø—É 5

#### 1. –°–æ–∑–¥–∞—Ç—å –µ–¥–∏–Ω—ã–π –º–µ—Ç–æ–¥ `_process_literal()`

**–î–æ–±–∞–≤–∏—Ç—å –≤ `processing/pipeline.py` –ø–æ—Å–ª–µ –º–µ—Ç–æ–¥–∞ `_process_profile`:**

```python
def _process_literal(
    self,
    context: ProcessingContext,
    node,
    profile: LiteralProfile,
    budget: int
) -> Optional[TrimResult]:
    """
    Unified literal processing entry point.

    Only coordinates stages and components - no detailed logic.

    Args:
        context: Processing context
        node: Tree-sitter node
        profile: Literal profile
        budget: Token budget

    Returns:
        TrimResult if optimization applied, None otherwise
    """
    # Check special components
    for component in self.special_components:
        if component.can_handle(profile, node, context.doc):
            result = component.process(
                node,
                context.doc,
                context.raw_text,
                profile,
                budget
            )
            # Handle tuple return from BlockInitProcessor
            if isinstance(result, tuple):
                trim_result, nodes_used = result
                return trim_result
            return result

    # Standard path through stages
    parsed = self.literal_parser.parse_from_node(
        node, context.doc, context.raw_text, profile
    )

    if not parsed or parsed.original_tokens <= budget:
        return None

    # Route by profile type
    if isinstance(profile, StringProfile):
        return self._process_string(parsed, budget)
    else:
        return self._process_collection(parsed, budget)
```

#### 2. –°–æ–∑–¥–∞—Ç—å —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π `_process_string()`

**–ó–∞–º–µ–Ω–∏—Ç—å —Ç–µ–∫—É—â–∏–π –º–µ—Ç–æ–¥ `_process_string()` –Ω–∞:**

```python
def _process_string(
    self,
    parsed: ParsedLiteral[StringProfile],
    budget: int
) -> Optional[TrimResult]:
    """
    Process string literals through standard stages.

    Args:
        parsed: Parsed string literal
        budget: Token budget

    Returns:
        TrimResult if optimization applied
    """
    # Calculate overhead
    overhead = self.budget_calculator.calculate_overhead(
        parsed.opening, parsed.closing, "‚Ä¶",
        parsed.is_multiline, parsed.element_indent
    )
    content_budget = max(1, budget - overhead)

    # Truncate content
    truncated = self.adapter.tokenizer.truncate_to_tokens(
        parsed.content, content_budget
    )

    if len(truncated) >= len(parsed.content):
        return None

    # Adjust for interpolation
    markers = self.interpolation.get_active_markers(
        parsed.profile, parsed.opening, parsed.content
    )
    if markers:
        truncated = self.interpolation.adjust_truncation(
            truncated, parsed.content, markers
        )

    # Create pseudo-selection and format
    kept_element = Element(
        text=truncated,
        raw_text=truncated,
        start_offset=0,
        end_offset=len(truncated),
    )
    removed_element = Element(
        text="...", raw_text="...",
        start_offset=0, end_offset=0
    )

    selection = Selection(
        kept_elements=[kept_element],
        removed_elements=[removed_element],
        total_count=1,
        tokens_kept=self.adapter.tokenizer.count_text_cached(truncated),
        tokens_removed=parsed.original_tokens - self.adapter.tokenizer.count_text_cached(truncated),
    )

    # Format result
    formatted = self.formatter.format(parsed, selection)
    return self.formatter.create_trim_result(parsed, selection, formatted)
```

#### 3. –°–æ–∑–¥–∞—Ç—å —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π `_process_collection()`

**–ó–∞–º–µ–Ω–∏—Ç—å —Ç–µ–∫—É—â–∏–π –º–µ—Ç–æ–¥ `_process_collection_dfs()` –Ω–∞ `_process_collection()`:**

```python
def _process_collection(
    self,
    parsed: ParsedLiteral[CollectionProfile],
    budget: int
) -> Optional[TrimResult]:
    """
    Process collections through selector + formatter.

    Args:
        parsed: Parsed collection literal
        budget: Token budget

    Returns:
        TrimResult if optimization applied
    """
    parser = self._get_parser_for_profile(parsed.profile)
    elements = parser.parse(parsed.content)

    if not elements:
        return None

    # Calculate overhead
    placeholder = parsed.profile.placeholder_template
    overhead = self.budget_calculator.calculate_overhead(
        parsed.opening, parsed.closing, placeholder,
        parsed.is_multiline, parsed.element_indent
    )
    content_budget = max(1, budget - overhead)

    # Select elements with DFS
    selection = self.selector.select_dfs(
        elements, content_budget,
        profile=parsed.profile,
        get_parser_func=self._get_parser_for_profile,
        min_keep=parsed.profile.min_elements,
        tuple_size=parsed.profile.tuple_size if isinstance(parsed.profile, FactoryProfile) else 1,
        preserve_top_level_keys=parsed.profile.preserve_all_keys if isinstance(parsed.profile, MappingProfile) else False,
    )

    if not selection.has_removals:
        return None

    # Format result
    formatted = self.formatter.format_dfs(parsed, selection, parser)

    trimmed_tokens = self.adapter.tokenizer.count_text_cached(formatted.text)

    return TrimResult(
        trimmed_text=formatted.text,
        original_tokens=parsed.original_tokens,
        trimmed_tokens=trimmed_tokens,
        saved_tokens=parsed.original_tokens - trimmed_tokens,
        elements_kept=selection.kept_count,
        elements_removed=selection.removed_count,
        comment_text=formatted.comment,
        comment_position=formatted.comment_byte,
    )
```

#### 4. –£–¥–∞–ª–∏—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–æ—É—Ç–µ—Ä—ã

**–£–¥–∞–ª–∏—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é —ç—Ç–∏ –º–µ—Ç–æ–¥—ã:**
- `_process_block_init_node()`
- `_process_sequence_node()`
- `_process_standard_collection_node()`
- `_process_literal_impl()`

#### 5. –û–±–Ω–æ–≤–∏—Ç—å `_process_profile()` –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è `_process_literal()`

**–ó–∞–º–µ–Ω–∏—Ç—å –≤ –º–µ—Ç–æ–¥–µ `_process_profile()` –≤—ã–∑–æ–≤ `processor(...)` –Ω–∞:**

```python
# –ë—ã–ª–æ:
result = processor(context, node, max_tokens, profile)

# –°—Ç–∞–ª–æ:
result = self._process_literal(context, node, profile, max_tokens)
```

–ò —É–¥–∞–ª–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä `processor` –∏–∑ —Å–∏–≥–Ω–∞—Ç—É—Ä—ã –º–µ—Ç–æ–¥–∞.

#### 6. –£–ø—Ä–æ—Å—Ç–∏—Ç—å `_process_strings()` –∏ `_process_collections()`

–û–±–Ω–æ–≤–∏—Ç—å —Ü–∏–∫–ª—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –µ–¥–∏–Ω–æ–≥–æ `_process_literal()`.

–í `_process_strings()`:
```python
# –ë—ã–ª–æ —Å–ª–æ–∂–Ω–æ–µ –≤–µ—Ç–≤–ª–µ–Ω–∏–µ —Å docstring checks
# –°—Ç–∞–ª–æ:
result = self._process_literal(context, node, profile, max_tokens)
```

–í `_process_collections()`:
```python
# –ë—ã–ª–æ:
self._process_profile(context, profile, max_tokens, processed_strings, processor)

# –°—Ç–∞–ª–æ:
self._process_profile(context, profile, max_tokens, processed_strings)
```

#### 7. –°–æ–∑–¥–∞—Ç—å –µ–¥–∏–Ω—ã–π –º–µ—Ç–æ–¥ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

**–û–±—ä–µ–¥–∏–Ω–∏—Ç—å `_apply_trim_result()` –∏ `_apply_trim_result_composing()` –≤ –æ–¥–∏–Ω:**

```python
def _apply_result(
    self,
    context: ProcessingContext,
    node,
    result: TrimResult,
    original_text: str,
    use_composing: bool = False
) -> None:
    """
    Unified result application.

    Args:
        context: Processing context
        node: Tree-sitter node
        result: Trim result to apply
        original_text: Original text for metrics
        use_composing: Whether to use composing method
    """
    start_byte, end_byte = context.doc.get_node_range(node)

    if use_composing:
        context.editor.add_replacement_composing_nested(
            start_byte, end_byte, result.trimmed_text,
            edit_type="literal_trimmed"
        )
    else:
        context.editor.add_replacement(
            start_byte, end_byte, result.trimmed_text,
            edit_type="literal_trimmed"
        )

    # Add comment if needed
    placeholder_style = self.adapter.cfg.placeholders.style
    if placeholder_style != "none" and result.comment_text:
        text_after = context.raw_text[end_byte:]
        formatted_comment, offset = self.formatter._format_comment_for_context(
            text_after, result.comment_text
        )
        context.editor.add_insertion(
            end_byte + offset,
            formatted_comment,
            edit_type="literal_comment"
        )

    # Update metrics
    context.metrics.mark_element_removed("literal")
    context.metrics.add_chars_saved(len(original_text) - len(result.trimmed_text))
```

---

### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≠—Ç–∞–ø–∞ 5

```bash
./scripts/test_adapters.sh literals all
# –û–∂–∏–¥–∞–Ω–∏–µ: 100 passed
```

–§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞:

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–∑–º–µ—Ä pipeline
wc -l lg/adapters/optimizations/literals/processing/pipeline.py
# –¶–µ–ª—å: ~250 —Å—Ç—Ä–æ–∫ (–±—ã–ª–æ ~700)

# –ü–æ–ª–Ω—ã–π –ø—Ä–æ–≥–æ–Ω –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤
./scripts/test_adapters.sh all all
# –û–∂–∏–¥–∞–Ω–∏–µ: 100+ passed, 0 failed
```

---

### –ö–æ–º–º–∏—Ç –≠—Ç–∞–ø–∞ 5

```bash
git add lg/adapters/optimizations/literals/
git commit -m "refactor(literals): Simplify pipeline to pure orchestrator

- Create unified _process_literal() method
- Simplify _process_string() and _process_collection()
- Remove specialized routing methods
- Delegate applicability checks to components via can_handle()
- Pipeline is now ~250 lines of pure coordination
- Clean separation: pipeline coordinates, components/stages execute

No behavioral changes, all tests pass."
```

---

## –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞

–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≠—Ç–∞–ø–∞ 5:

### 1. –ü–æ–ª–Ω—ã–π –ø—Ä–æ–≥–æ–Ω —Ç–µ—Å—Ç–æ–≤ (–µ—Å–ª–∏ –µ—â–µ –Ω–µ –±—ã–ª —Å–¥–µ–ª–∞–Ω)

```bash
./scripts/test_adapters.sh literals all
# –û–∂–∏–¥–∞–Ω–∏–µ: 100+ passed, 0 failed
```

### 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã

```bash
ls -R lg/adapters/optimizations/literals/

# –û–∂–∏–¥–∞–µ–º–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:
# processing/ - 4 —Ñ–∞–π–ª–∞ (pipeline, parser, selector, formatter)
# components/ - 2 —Ñ–∞–π–ª–∞ (ast_sequence, block_init)
# utils/ - 3 —Ñ–∞–π–ª–∞ (element_parser, budgeting, interpolation)
# –ö–æ—Ä–µ–Ω—å - –º–æ–¥–µ–ª—å (descriptor, patterns, __init__)
```

### 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ pipeline.py

```bash
wc -l lg/adapters/optimizations/literals/processing/pipeline.py
# –û–∂–∏–¥–∞–Ω–∏–µ: ~250 —Å—Ç—Ä–æ–∫ (–±—ã–ª–æ ~700)
```

### 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ git —Å—Ç–∞—Ç—É—Å–∞

```bash
git status
# –û–∂–∏–¥–∞–Ω–∏–µ: working tree clean (–≤—Å–µ –∑–∞–∫–æ–º–º–∏—á–µ–Ω–æ)
```

---

## –ú–µ—Ç—Ä–∏–∫–∏ —É—Å–ø–µ—Ö–∞

### –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏

- ‚úÖ `pipeline.py`: ~250 —Å—Ç—Ä–æ–∫ (–±—ã–ª–æ ~700)
- ‚úÖ –¢–æ–ª—å–∫–æ 2 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –≤ `components/`
- ‚úÖ 3 —É—Ç–∏–ª–∏—Ç—ã –≤ `utils/`
- ‚úÖ 100+ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ—Ö–æ–¥—è—Ç
- ‚úÖ 0 –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ golden files

### –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏

- ‚úÖ Pipeline –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–µ—Ç–∞–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
- ‚úÖ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∞–≤—Ç–æ–Ω–æ–º–Ω—ã (can_handle + process)
- ‚úÖ –°—Ç–∞–¥–∏–∏ –∏–º–µ—é—Ç –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π API
- ‚úÖ –ß–µ—Ç–∫–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ processing/components/utils
- ‚úÖ –õ–µ–≥–∫–æ –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏ —è–∑—ã–∫–∏
