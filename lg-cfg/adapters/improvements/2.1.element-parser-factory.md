# Избыточное создание ElementParser

## Проблема

В `StandardCollectionsProcessor` кэш парсеров (`_parsers`) работает локально на инстанс компонента, но компонент создается заново для каждого файла в `LiteralPipeline.__init__()`. Это приводит к потере кэша и повторному созданию ElementParser при обработке множества файлов.

## Анализ текущего поведения

### Текущая архитектура

```python
# processing/pipeline.py
class LiteralPipeline:
    def __init__(self, cfg: LiteralConfig, adapter):
        # ...
        # Компоненты создаются КАЖДЫЙ РАЗ при создании pipeline
        # А pipeline создается для КАЖДОГО файла в адаптере
        self.special_components: List[LiteralProcessor] = [
            ASTSequenceProcessor(...),
            JavaDoubleBraceProcessor(...),
            RustLetGroupProcessor(...),
            StringLiteralProcessor(...),
            StandardCollectionsProcessor(  # Создается заново!
                self.adapter.tokenizer,
                self.literal_parser,
                self.selector,
                self.comment_formatter,
                self.descriptor
            ),
        ]

# components/standard_collections.py
class StandardCollectionsProcessor(LiteralProcessor):
    def __init__(
        self,
        tokenizer: TokenService,
        literal_parser: LiteralParser,
        selector: BudgetSelector,
        comment_formatter: CommentFormatter,
        descriptor: LanguageLiteralDescriptor,
    ):
        self.tokenizer = tokenizer
        self.parser = literal_parser
        self.selector = selector
        self.collection_formatter = CollectionFormatter(tokenizer, comment_formatter)
        self.descriptor = descriptor
        # Кэш парсеров - локальный для этого инстанса!
        self._parsers: dict[str, ElementParser] = {}  # ТЕРЯЕТСЯ при пересоздании компонента!

    def _get_parser_for_profile(self, profile: CollectionProfile) -> ElementParser:
        # ...
        key = f"{separator}:{kv_separator}:{tuple_size}"

        if key not in self._parsers:
            # Создается заново для каждого файла!
            config = ParseConfig.from_profile_and_descriptor(profile, self.descriptor)
            self._parsers[key] = ElementParser(config)

        return self._parsers[key]
```

### Проблема масштабирования

**Сценарий**: обработка 100 файлов Python с литералами

1. Файл 1: создается `LiteralPipeline` → создается `StandardCollectionsProcessor` → `_parsers = {}`
2. При обработке литералов создаются парсеры для каждого профиля
3. Файл 2: создается НОВЫЙ `LiteralPipeline` → создается НОВЫЙ `StandardCollectionsProcessor` → `_parsers = {}` (ПУСТОЙ!)
4. Парсеры создаются ЗАНОВО для тех же профилей
5. ... × 100 файлов

**Итог**: ElementParser для одного и того же профиля создается 100 раз вместо 1 раза.

## Решение: ElementParserFactory как shared сервис

### Новый модуль utils/parser_factory.py

```python
"""Factory for creating and caching ElementParser instances."""

from __future__ import annotations

from typing import Dict, List, Optional

from .element_parser import ElementParser, ParseConfig


class ElementParserFactory:
    """
    Singleton factory for ElementParser instances.

    Caches parsers across all file processing to avoid redundant
    creation of identical parsers for the same profile configurations.
    """

    def __init__(self):
        """Initialize factory with empty cache."""
        self._cache: Dict[str, ElementParser] = {}

    def get_parser(
        self,
        separator: str,
        kv_separator: Optional[str],
        tuple_size: int,
        factory_wrappers: List[str],
    ) -> ElementParser:
        """
        Get or create ElementParser for given configuration.

        Args:
            separator: Element separator (e.g., ",")
            kv_separator: Key-value separator for mappings (e.g., ":", " -> ")
            tuple_size: Tuple grouping size for factory methods
            factory_wrappers: List of factory wrapper names for nested detection

        Returns:
            Cached or newly created ElementParser

        Example:
            >>> factory = ElementParserFactory()
            >>> parser1 = factory.get_parser(",", ":", 1, ["List.of"])
            >>> parser2 = factory.get_parser(",", ":", 1, ["List.of"])
            >>> assert parser1 is parser2  # Same instance from cache
        """
        # Create stable cache key
        key = self._make_cache_key(separator, kv_separator, tuple_size, factory_wrappers)

        if key not in self._cache:
            # Create new parser
            config = ParseConfig(
                separator=separator,
                kv_separator=kv_separator,
                factory_wrappers=factory_wrappers,
            )
            self._cache[key] = ElementParser(config)

        return self._cache[key]

    def _make_cache_key(
        self,
        separator: str,
        kv_separator: Optional[str],
        tuple_size: int,
        factory_wrappers: List[str],
    ) -> str:
        """
        Create stable cache key from parser configuration.

        Uses sorted factory_wrappers to ensure stable key regardless of order.
        """
        # Sort wrappers for stable key
        sorted_wrappers = tuple(sorted(factory_wrappers))
        wrappers_hash = hash(sorted_wrappers)

        return f"{separator}:{kv_separator}:{tuple_size}:{wrappers_hash}"

    def clear_cache(self):
        """Clear parser cache. Useful for testing."""
        self._cache.clear()

    def cache_size(self) -> int:
        """Get current cache size. Useful for monitoring."""
        return len(self._cache)
```

### Интеграция в LiteralPipeline

```python
# processing/pipeline.py
from ..utils.parser_factory import ElementParserFactory


class LiteralPipeline:
    """
    Main pipeline for literal optimization.

    Orchestrates single-pass unified processing for all literal types.
    """

    def __init__(self, cfg: LiteralConfig, adapter):
        """
        Initialize pipeline.

        Args:
            cfg: Literal configuration
            adapter: Language adapter
        """
        self.cfg = cfg
        from ....code_base import CodeAdapter
        self.adapter = cast(CodeAdapter, adapter)

        # Get descriptor from adapter
        self.descriptor = self.adapter.create_literal_descriptor()

        # Get comment style from adapter
        comment_style: tuple[str, tuple[str, str]] = cast(
            tuple[str, tuple[str, str]], self.adapter.get_comment_style()[:2]
        )

        # ===== Shared services =====
        self.selector = BudgetSelector(self.adapter.tokenizer)
        self.comment_formatter = CommentFormatter(comment_style)
        self.literal_parser = LiteralParser(self.adapter.tokenizer)

        # NEW: Shared parser factory for ElementParser caching
        self.parser_factory = ElementParserFactory()

        # ===== Processing components =====
        self.special_components: List[LiteralProcessor] = [
            # Special cases
            ASTSequenceProcessor(
                self.adapter.tokenizer,
                [p for p in self.descriptor.profiles if isinstance(p, StringProfile)]
            ),
            JavaDoubleBraceProcessor(
                self.adapter.tokenizer,
                self.descriptor.profiles,
                self._process_literal,
                comment_style
            ),
            RustLetGroupProcessor(
                self.adapter.tokenizer,
                self.descriptor.profiles,
                self._process_literal,
                comment_style
            ),

            # Standard cases
            StringLiteralProcessor(
                self.adapter.tokenizer,
                self.literal_parser,
                self.comment_formatter
            ),
            StandardCollectionsProcessor(
                self.adapter.tokenizer,
                self.literal_parser,
                self.selector,
                self.comment_formatter,
                self.descriptor,
                self.parser_factory,  # Передаем shared factory
            ),
        ]
```

### Обновление StandardCollectionsProcessor

```python
# components/standard_collections.py
from ..utils.parser_factory import ElementParserFactory


class StandardCollectionsProcessor(LiteralProcessor):
    """
    Processes standard collection literals with DFS optimization.

    Autonomous component that:
    - Parses collection structure
    - Applies DFS selection with budget
    - Formats result with nested handling
    """

    def __init__(
        self,
        tokenizer: TokenService,
        literal_parser: LiteralParser,
        selector: BudgetSelector,
        comment_formatter: CommentFormatter,
        descriptor: LanguageLiteralDescriptor,
        parser_factory: ElementParserFactory,  # NEW: принимаем фабрику
    ):
        """
        Initialize processor.

        Args:
            tokenizer: Token counting service
            literal_parser: Shared LiteralParser instance
            selector: BudgetSelector instance
            comment_formatter: Shared CommentFormatter instance
            descriptor: Language literal descriptor (for ElementParser factory)
            parser_factory: Shared ElementParserFactory instance
        """
        self.tokenizer = tokenizer
        self.parser = literal_parser
        self.selector = selector
        self.collection_formatter = CollectionFormatter(tokenizer, comment_formatter)
        self.descriptor = descriptor
        self.parser_factory = parser_factory  # Сохраняем фабрику

        # Предвычислить factory_wrappers один раз
        self.factory_wrappers = ElementParser.collect_factory_wrappers_from_descriptor(
            self.descriptor
        )

        # УДАЛЕНО: self._parsers = {}  # Больше не нужен локальный кэш!

    def _get_parser_for_profile(self, profile: CollectionProfile) -> ElementParser:
        """
        Get parser for a profile via shared factory.

        Uses shared ElementParserFactory to get cached parser,
        avoiding redundant parser creation across files.

        Args:
            profile: CollectionProfile to create parser for

        Returns:
            ElementParser configured for this profile
        """
        separator = profile.separator
        kv_separator = profile.kv_separator if isinstance(profile, (MappingProfile, FactoryProfile)) else None
        tuple_size = profile.tuple_size if isinstance(profile, FactoryProfile) else 1

        # Используем shared factory вместо локального кэша
        return self.parser_factory.get_parser(
            separator=separator,
            kv_separator=kv_separator,
            tuple_size=tuple_size,
            factory_wrappers=self.factory_wrappers,  # Переиспользуем предвычисленное
        )
```

## Тестирование

```python
# tests/adapters/test_parser_factory.py
import pytest
from lg.adapters.optimizations.literals.utils import ElementParserFactory


class TestElementParserFactory:
    def test_same_config_returns_same_instance(self):
        """Same configuration should return cached instance."""
        factory = ElementParserFactory()

        parser1 = factory.get_parser(",", ":", 1, ["List.of"])
        parser2 = factory.get_parser(",", ":", 1, ["List.of"])

        assert parser1 is parser2  # Same object

    def test_different_separator_creates_different_parser(self):
        """Different separator should create different parser."""
        factory = ElementParserFactory()

        parser1 = factory.get_parser(",", ":", 1, [])
        parser2 = factory.get_parser(";", ":", 1, [])

        assert parser1 is not parser2

    def test_different_kv_separator_creates_different_parser(self):
        """Different kv_separator should create different parser."""
        factory = ElementParserFactory()

        parser1 = factory.get_parser(",", ":", 1, [])
        parser2 = factory.get_parser(",", " -> ", 1, [])

        assert parser1 is not parser2

    def test_factory_wrappers_order_ignored(self):
        """Factory wrappers order should not affect caching."""
        factory = ElementParserFactory()

        parser1 = factory.get_parser(",", ":", 1, ["Map.of", "List.of"])
        parser2 = factory.get_parser(",", ":", 1, ["List.of", "Map.of"])

        assert parser1 is parser2  # Same instance despite different order

    def test_cache_size(self):
        """Cache size should reflect number of unique configurations."""
        factory = ElementParserFactory()

        assert factory.cache_size() == 0

        factory.get_parser(",", ":", 1, [])
        assert factory.cache_size() == 1

        factory.get_parser(",", ":", 1, [])  # Same config - no new entry
        assert factory.cache_size() == 1

        factory.get_parser(";", ":", 1, [])  # Different config - new entry
        assert factory.cache_size() == 2

    def test_clear_cache(self):
        """Clear cache should remove all cached parsers."""
        factory = ElementParserFactory()

        factory.get_parser(",", ":", 1, [])
        factory.get_parser(";", ":", 1, [])
        assert factory.cache_size() == 2

        factory.clear_cache()
        assert factory.cache_size() == 0
```

## Измерение эффекта

```python
# scripts/benchmark_parser_factory.py
"""Benchmark script to measure parser factory performance impact."""

import time
from pathlib import Path
from lg.adapters.optimizations.literals.utils import ElementParserFactory
from lg.adapters.python.literals import create_python_descriptor


def benchmark_without_factory(n_files: int):
    """Simulate old behavior: create parser for each file."""
    descriptor = create_python_descriptor()

    start = time.time()
    for _ in range(n_files):
        # Simulate creating parsers for each file
        parsers = {}
        for profile in descriptor.profiles:
            # Создаем parser каждый раз
            config = ParseConfig.from_profile_and_descriptor(profile, descriptor)
            parsers[profile.query] = ElementParser(config)

    elapsed = time.time() - start
    print(f"WITHOUT factory ({n_files} files): {elapsed:.3f}s")
    return elapsed


def benchmark_with_factory(n_files: int):
    """Simulate new behavior: shared factory."""
    descriptor = create_python_descriptor()
    factory = ElementParserFactory()

    start = time.time()
    for _ in range(n_files):
        # Используем фабрику - парсеры кэшируются
        for profile in descriptor.profiles:
            parser = factory.get_parser(
                separator=profile.separator,
                kv_separator=getattr(profile, 'kv_separator', None),
                tuple_size=getattr(profile, 'tuple_size', 1),
                factory_wrappers=[],
            )

    elapsed = time.time() - start
    print(f"WITH factory ({n_files} files): {elapsed:.3f}s")
    return elapsed


if __name__ == "__main__":
    n_files = 100

    without = benchmark_without_factory(n_files)
    with_factory = benchmark_with_factory(n_files)

    speedup = without / with_factory
    print(f"\nSpeedup: {speedup:.2f}x")
```

## Ожидаемые выгоды

- Кэш ElementParser переиспользуется между файлами
- Снижение накладных расходов на создание ElementParser
- Более явная архитектура shared сервисов
- Улучшение производительности при обработке множества файлов (ожидаемое ускорение ~2-5x для parser creation)
- Упрощение тестирования (можно мокировать фабрику)
