# –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ: –ü–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–¥—Å–∏—Å—Ç–µ–º—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤ LG

## 1. –ë–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

### –ü—Ä–æ–±–ª–µ–º—ã —Ç–µ–∫—É—â–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

1. **–£—Å—Ç–∞—Ä–µ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏**: `lg-cfg/models.yaml` —Å —Ö–∞—Ä–¥–∫–æ–¥-—Å–ø–∏—Å–∫–æ–º –º–æ–¥–µ–ª–µ–π –∏ –ø–ª–∞–Ω–æ–≤ –±—ã—Å—Ç—Ä–æ —É—Å—Ç–∞—Ä–µ–≤–∞–µ—Ç –∏–∑-–∑–∞ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –∏–Ω–¥—É—Å—Ç—Ä–∏–∏ LLM.

2. **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å**: –ü—Ä–∏–≤—è–∑–∫–∞ –∫ `tiktoken` (OpenAI) –Ω–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º–∏ –¥—Ä—É–≥–∏—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ (Anthropic, Google, xAI).

3. **–°–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ª–∏–º–∏—Ç–æ–≤**: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ —á–µ—Ä–µ–∑ "–ø–ª–∞–Ω—ã –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤" –∏–∑–±—ã—Ç–æ—á–Ω–æ –∏ —Ç—Ä—É–¥–Ω–æ –∞–∫—Ç—É–∞–ª–∏–∑–∏—Ä—É–µ–º–æ.

### –¶–µ–ª–µ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ

**–Ø–≤–Ω—ã–π API –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏**: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫—É, —ç–Ω–∫–æ–¥–µ—Ä –∏ —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ —á–µ—Ä–µ–∑ CLI-–∞—Ä–≥—É–º–µ–Ω—Ç—ã –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∑–∞–ø—É—Å–∫–µ. –≠—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:

- **–ü—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å**: –Ω–µ—Ç —Å–∫—Ä—ã—Ç–æ–π –º–∞–≥–∏–∏, –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —è–≤–Ω—ã–µ
- **–ì–∏–±–∫–æ—Å—Ç—å**: –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
- **–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å**: –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ LG –ø—Ä–∏ –ø–æ—è–≤–ª–µ–Ω–∏–∏ –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
- **–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å**: —Ä–∞–±–æ—Ç–∞ —Å –ª—é–±—ã–º–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏ —á–µ—Ä–µ–∑ –æ–ø–µ–Ω—Å–æ—Ä—Å–Ω—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã

### –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

LG –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ —á–µ—Ä–µ–∑ IDE-–∞–¥–¥–æ–Ω—ã (VS Code, JetBrains), –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç:
- –†–∞–∑–≤–∏—Ç—ã–π UI –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
- –°–∏—Å—Ç–µ–º—É –ø—Ä–æ—Ñ–∏–ª–µ–π –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
- –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —á–∞—Å—Ç–æ–≥–æ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –±–µ–∑ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è YAML

–ü–æ—ç—Ç–æ–º—É verbose CLI (3 –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –≤–º–µ—Å—Ç–æ 1) –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–æ–π –¥–ª—è UX.

---

## 2. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

### 2.1. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏

–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º —Ç—Ä–∏ –æ–ø–µ–Ω—Å–æ—Ä—Å–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:

1. **`tiktoken`** (OpenAI) - GPT-–º–æ–¥–µ–ª–∏, –±—ã—Å—Ç—Ä—ã–π C-–∞–¥–∞–ø—Ç–µ—Ä
2. **`tokenizers`** (HuggingFace) - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è Rust-–±–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
3. **`sentencepiece`** (Google) - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ Gemini –∏ –º–Ω–æ–≥–∏—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º–æ–¥–µ–ª—è—Ö

**–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ**: —ç—Ç–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –ø–æ–∫—Ä—ã–≤–∞—é—Ç >90% –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –î–ª—è –∑–∞–∫—Ä—ã—Ç—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ (Claude, Grok) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è approximation —á–µ—Ä–µ–∑ –ø–æ—Ö–æ–∂–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã.

### 2.2. –ù–æ–≤–æ–µ CLI

#### –£–¥–∞–ª—è–µ–º—ã–µ –∫–æ–º–∞–Ω–¥—ã

```bash
# –£–î–ê–õ–ò–¢–¨
lg report ctx:all --model gpt-4o
lg list models
```

#### –ù–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã

```bash
# –°–ø–∏—Å–æ–∫ –±–∏–±–ª–∏–æ—Ç–µ–∫
lg list tokenizer-libs
# ‚Üí ["tiktoken", "tokenizers", "sentencepiece"]

# –°–ø–∏—Å–æ–∫ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –¥–ª—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
lg list encoders --lib tiktoken
lg list encoders --lib tokenizers
lg list encoders --lib sentencepiece

# –†–µ–Ω–¥–µ—Ä–∏–Ω–≥/–æ—Ç—á–µ—Ç —Å —è–≤–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (–≤—Å–µ 3 –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã)
lg render ctx:all --lib tiktoken --encoder cl100k_base --ctx-limit 128000
lg report sec:core --lib sentencepiece --encoder google/gemma-2-2b --ctx-limit 1000000
```

### 2.3. –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

- **–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è**: `lg-cfg/tokenizer-models/` (–≤–Ω—É—Ç—Ä–∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è git)
- **–°—Ç—Ä—É–∫—Ç—É—Ä–∞**: `lg-cfg/tokenizer-models/{lib}/{model_name}/`
- **–ê–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–∞**: –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏ —Å–∫–∞—á–∏–≤–∞—é—Ç—Å—è —Å HuggingFace Hub
- **–û—Ç–¥–µ–ª—å–Ω—ã–π –º–æ–¥—É–ª—å**: `lg/stats/model_cache.py` (–Ω–µ —Å–º–µ—à–∏–≤–∞—Ç—å —Å `lg/cache/fs_cache.py`)

### 2.4. –û–±—Ä–∞–±–æ—Ç–∫–∞ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤

#### `tiktoken`
- –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —ç–Ω–∫–æ–¥–µ—Ä—ã: `gpt2`, `r50k_base`, `p50k_base`, `cl100k_base`, `o200k_base`
- –°–ø–∏—Å–æ–∫ —á–µ—Ä–µ–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π API: `tiktoken.list_encoding_names()`
- –ù–µ —Ç—Ä–µ–±—É—é—Ç —Å–∫–∞—á–∏–≤–∞–Ω–∏—è

#### `tokenizers` (HuggingFace)
- –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏ (—Ö–∞—Ä–¥–∫–æ–¥-—Å–ø–∏—Å–æ–∫): `gpt2`, `roberta-base`, `bert-base-uncased`, `bert-base-cased`, `t5-base`, `google/gemma-tokenizer`
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ª—é–±—ã—Ö –º–æ–¥–µ–ª–µ–π —Å HF Hub —á–µ—Ä–µ–∑ `Tokenizer.from_pretrained(model_name)`
- –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ `lg-cfg/tokenizer-models/tokenizers/{model_name}/`

#### `sentencepiece`
- –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏ (—Ö–∞—Ä–¥–∫–æ–¥-—Å–ø–∏—Å–æ–∫): `google/gemma-2-2b`, `meta-llama/Llama-2-7b-hf`
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö `.spm` —Ñ–∞–π–ª–æ–≤: `--encoder /path/to/model.spm`
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ª—é–±—ã—Ö –º–æ–¥–µ–ª–µ–π —Å HF Hub
- –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ `lg-cfg/tokenizer-models/sentencepiece/{model_name}/`

### 2.5. JSON —Å—Ö–µ–º–∞ –æ—Ç—á–µ—Ç–∞ (protocol v5)

```json
{
  "protocol": 5,
  "tokenizerLib": "tiktoken",
  "encoder": "o200k_base",
  "ctxLimit": 200000,
  "scope": "context",
  "target": "ctx:all",
  "total": { ... },
  "files": [ ... ],
  "context": { ... }
}
```

**–ò–∑–º–µ–Ω–µ–Ω–∏—è**:
- `protocol`: 4 ‚Üí 5
- **–£–î–ê–õ–ï–ù–û**: –ø–æ–ª–µ `model`
- **–î–û–ë–ê–í–õ–ï–ù–û**: `tokenizerLib` (—Å—Ç—Ä–æ–∫–∞: `tiktoken|tokenizers|sentencepiece`)
- **–°–û–•–†–ê–ù–ï–ù–û**: `encoder`, `ctxLimit`

---

## 3. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ä–µ—à–µ–Ω–∏—è

### 3.1. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–æ–¥—É–ª—è `lg/stats/`

#### –£–¥–∞–ª—è–µ–º—ã–µ —Ñ–∞–π–ª—ã

```
lg/stats/
‚îú‚îÄ‚îÄ load.py          ‚ùå –£–î–ê–õ–ò–¢–¨ –ü–û–õ–ù–û–°–¢–¨–Æ
‚îú‚îÄ‚îÄ model.py         ‚ùå –£–î–ê–õ–ò–¢–¨ –ü–û–õ–ù–û–°–¢–¨–Æ
```

#### –ù–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞

```
lg/stats/
‚îú‚îÄ‚îÄ __init__.py                      ‚öôÔ∏è –û–±–Ω–æ–≤–∏—Ç—å —ç–∫—Å–ø–æ—Ä—Ç—ã
‚îú‚îÄ‚îÄ collector.py                     ‚úÖ –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
‚îú‚îÄ‚îÄ report_builder.py                üìù –£–ø—Ä–æ—Å—Ç–∏—Ç—å (—É–±—Ä–∞—Ç—å –ª–æ–≥–∏–∫—É –ø–ª–∞–Ω–æ–≤)
‚îú‚îÄ‚îÄ report_schema.py                 üìù –û–±–Ω–æ–≤–∏—Ç—å Pydantic-–º–æ–¥–µ–ª–∏
‚îú‚îÄ‚îÄ tokenizers/                      ‚≠ê –ù–û–í–´–ô –ü–û–î–ú–û–î–£–õ–¨
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                  - –§–∞–±—Ä–∏–∫–∞ –∏ –ø—É–±–ª–∏—á–Ω—ã–π API
‚îÇ   ‚îú‚îÄ‚îÄ base.py                      - –ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –∫–ª–∞—Å—Å BaseTokenizer
‚îÇ   ‚îú‚îÄ‚îÄ tiktoken_adapter.py          - –ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è tiktoken
‚îÇ   ‚îú‚îÄ‚îÄ hf_adapter.py                - –ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è HuggingFace tokenizers
‚îÇ   ‚îú‚îÄ‚îÄ sp_adapter.py                - –ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è SentencePiece
‚îÇ   ‚îî‚îÄ‚îÄ model_cache.py               - –ú–µ–Ω–µ–¥–∂–µ—Ä –∫–µ—à–∞ –º–æ–¥–µ–ª–µ–π
```

### 3.2. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```toml
# pyproject.toml

[project]
dependencies = [
    "ruamel.yaml>=0.18",
    "pathspec>=0.12",
    "tiktoken>=0.6",
    "tokenizers>=0.15",           # ‚≠ê –ù–û–í–û–ï
    "sentencepiece>=0.2",         # ‚≠ê –ù–û–í–û–ï
    "huggingface-hub>=0.20",      # ‚≠ê –ù–û–í–û–ï (–¥–ª—è –∞–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–∏)
    "pydantic>=2.0,<3.0",
    "tree-sitter>=0.21",
    # ...
]
```

### 3.3. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ CLI

```python
# lg/cli.py

# –£–î–ê–õ–ò–¢–¨ –æ–ø—Ü–∏—é --model –∏–∑ –≤—Å–µ—Ö –∫–æ–º–∞–Ω–¥
# –î–û–ë–ê–í–ò–¢–¨ –Ω–æ–≤—ã–µ –æ–ø—Ü–∏–∏ –¥–ª—è render/report:
#   --lib <tiktoken|tokenizers|sentencepiece>
#   --encoder <encoder_name>
#   --ctx-limit <int>

# –£–î–ê–õ–ò–¢–¨ –ø–æ–¥–∫–æ–º–∞–Ω–¥—É lg list models

# –î–û–ë–ê–í–ò–¢–¨ –Ω–æ–≤—ã–µ –ø–æ–¥–∫–æ–º–∞–Ω–¥—ã:
#   lg list tokenizer-libs
#   lg list encoders --lib <lib_name>
```

---

## 4. –î–µ—Ç–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è

### 4.1. –ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å

**–§–∞–π–ª**: `lg/stats/tokenizers/base.py`

```python
from abc import ABC, abstractmethod
from typing import List

class BaseTokenizer(ABC):
    """
    –ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤.
    
    –£–Ω–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞–º–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.
    """
    
    def __init__(self, encoder: str, ctx_limit: int):
        """
        Args:
            encoder: –ò–º—è —ç–Ω–∫–æ–¥–µ—Ä–∞ (–¥–ª—è tiktoken) –∏–ª–∏ –º–æ–¥–µ–ª–∏ (–¥–ª—è HF/SP)
            ctx_limit: –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö
        """
        self.encoder = encoder
        self.ctx_limit = ctx_limit
    
    @abstractmethod
    def count_tokens(self, text: str) -> int:
        """
        –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ.
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
        """
        pass
    
    @abstractmethod
    def encode(self, text: str) -> List[int]:
        """
        –ö–æ–¥–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ —Å–ø–∏—Å–æ–∫ token IDs.
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –°–ø–∏—Å–æ–∫ token IDs
        """
        pass
    
    @abstractmethod
    def decode(self, token_ids: List[int]) -> str:
        """
        –î–µ–∫–æ–¥–∏—Ä—É–µ—Ç token IDs –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç.
        
        Args:
            token_ids: –°–ø–∏—Å–æ–∫ token IDs
            
        Returns:
            –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        pass
    
    @staticmethod
    @abstractmethod
    def list_available_encoders() -> List[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –¥–ª—è –¥–∞–Ω–Ω–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏.
        
        –í–∫–ª—é—á–∞–µ—Ç:
        - –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —ç–Ω–∫–æ–¥–µ—Ä—ã (–¥–ª—è tiktoken)
        - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏ (–¥–ª—è HF/SP)
        - –£–∂–µ —Å–∫–∞—á–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∏–º–µ–Ω —ç–Ω–∫–æ–¥–µ—Ä–æ–≤/–º–æ–¥–µ–ª–µ–π
        """
        pass
    
    @property
    def lib_name(self) -> str:
        """–ò–º—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (tiktoken, tokenizers, sentencepiece)."""
        return self.__class__.__name__.replace("Adapter", "").lower()
    
    @property
    def full_name(self) -> str:
        """–ü–æ–ª–Ω–æ–µ –∏–º—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ 'lib:encoder'."""
        return f"{self.lib_name}:{self.encoder}"
```

---

### 4.2. –ú–µ–Ω–µ–¥–∂–µ—Ä –∫–µ—à–∞ –º–æ–¥–µ–ª–µ–π

**–§–∞–π–ª**: `lg/stats/tokenizers/model_cache.py`

```python
from pathlib import Path
from typing import Optional
import logging

logger = logging.getLogger(__name__)

class ModelCache:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –∫–µ—à–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.
    
    –•—Ä–∞–Ω–∏—Ç –º–æ–¥–µ–ª–∏ –≤ lg-cfg/tokenizer-models/{lib}/{model_name}/
    """
    
    def __init__(self, root: Path):
        """
        Args:
            root: –ö–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ (–≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è lg-cfg/)
        """
        self.root = root
        self.cache_dir = root / "lg-cfg" / "tokenizer-models"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ .gitignore –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        self._ensure_gitignore()
    
    def get_lib_cache_dir(self, lib: str) -> Path:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∫–µ—à–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏."""
        lib_dir = self.cache_dir / lib
        lib_dir.mkdir(parents=True, exist_ok=True)
        return lib_dir
    
    def get_model_cache_dir(self, lib: str, model_name: str) -> Path:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∫–µ—à–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏.
        
        Args:
            lib: –ò–º—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ (tokenizers, sentencepiece)
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏ (–º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å /, –Ω–∞–ø—Ä–∏–º–µ—Ä google/gemma-2-2b)
        
        Returns:
            –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∫–µ—à–∞ –º–æ–¥–µ–ª–∏
        """
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏ –≤ –ø—É—Ç—å
        safe_name = model_name.replace("/", "--")
        model_dir = self.get_lib_cache_dir(lib) / safe_name
        model_dir.mkdir(parents=True, exist_ok=True)
        return model_dir
    
    def is_model_cached(self, lib: str, model_name: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∑–∞–∫–µ—à–∏—Ä–æ–≤–∞–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å.
        
        Args:
            lib: –ò–º—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏
            
        Returns:
            True –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –µ—Å—Ç—å –≤ –∫–µ—à–µ
        """
        model_dir = self.get_model_cache_dir(lib, model_name)
        
        # –î–ª—è tokenizers –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ tokenizer.json
        if lib == "tokenizers":
            return (model_dir / "tokenizer.json").exists()
        
        # –î–ª—è sentencepiece –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ .model —Ñ–∞–π–ª–∞
        if lib == "sentencepiece":
            return any(model_dir.glob("*.model"))
        
        return False
    
    def list_cached_models(self, lib: str) -> list[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–∞–∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏.
        
        Args:
            lib: –ò–º—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∏–º–µ–Ω –º–æ–¥–µ–ª–µ–π
        """
        lib_dir = self.get_lib_cache_dir(lib)
        
        models = []
        for model_dir in lib_dir.iterdir():
            if not model_dir.is_dir():
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏
            if lib == "tokenizers" and (model_dir / "tokenizer.json").exists():
                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è
                original_name = model_dir.name.replace("--", "/")
                models.append(original_name)
            elif lib == "sentencepiece" and any(model_dir.glob("*.model")):
                original_name = model_dir.name.replace("--", "/")
                models.append(original_name)
        
        return sorted(models)
    
    def _ensure_gitignore(self) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç tokenizer-models/ –≤ lg-cfg/.gitignore –µ—Å–ª–∏ –Ω—É–∂–Ω–æ."""
        gitignore_path = self.cache_dir.parent / ".gitignore"
        entry = "tokenizer-models/\n"
        
        if gitignore_path.exists():
            content = gitignore_path.read_text(encoding="utf-8")
            if "tokenizer-models" not in content:
                gitignore_path.write_text(content + entry, encoding="utf-8")
        else:
            gitignore_path.write_text(entry, encoding="utf-8")
```

---

### 4.3. –ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è tiktoken

**–§–∞–π–ª**: `lg/stats/tokenizers/tiktoken_adapter.py`

```python
import tiktoken
from typing import List
from .base import BaseTokenizer

class TiktokenAdapter(BaseTokenizer):
    """–ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ tiktoken (OpenAI)."""
    
    def __init__(self, encoder: str, ctx_limit: int):
        super().__init__(encoder, ctx_limit)
        
        try:
            self._enc = tiktoken.get_encoding(encoder)
        except Exception as e:
            available = tiktoken.list_encoding_names()
            raise ValueError(
                f"Unknown tiktoken encoding '{encoder}'. "
                f"Available: {', '.join(available)}"
            ) from e
    
    def count_tokens(self, text: str) -> int:
        if not text:
            return 0
        return len(self._enc.encode(text))
    
    def encode(self, text: str) -> List[int]:
        return self._enc.encode(text)
    
    def decode(self, token_ids: List[int]) -> str:
        return self._enc.decode(token_ids)
    
    @staticmethod
    def list_available_encoders() -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ tiktoken."""
        return tiktoken.list_encoding_names()
```

---

### 4.4. –ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è HuggingFace tokenizers

**–§–∞–π–ª**: `lg/stats/tokenizers/hf_adapter.py`

```python
from pathlib import Path
from typing import List
import logging

from tokenizers import Tokenizer
from huggingface_hub import hf_hub_download

from .base import BaseTokenizer
from .model_cache import ModelCache

logger = logging.getLogger(__name__)

# –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã (–Ω–µ –ø—Ä–∏–≤—è–∑–∞–Ω—ã –∫ –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–º LLM)
RECOMMENDED_TOKENIZERS = [
    "gpt2",                    # GPT-2 BPE
    "roberta-base",            # RoBERTa BPE
    "bert-base-uncased",       # BERT WordPiece
    "bert-base-cased",         # BERT WordPiece (case-sensitive)
    "t5-base",                 # T5 SentencePiece-based
    "google/gemma-tokenizer",  # Gemma (Google)
]

class HFAdapter(BaseTokenizer):
    """–ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ tokenizers (HuggingFace)."""
    
    def __init__(self, encoder: str, ctx_limit: int, root: Path):
        super().__init__(encoder, ctx_limit)
        self.root = root
        self.model_cache = ModelCache(root)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        self._tokenizer = self._load_tokenizer(encoder)
    
    def _load_tokenizer(self, model_name: str) -> Tokenizer:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –∫–µ—à–∞ –∏–ª–∏ HuggingFace Hub.
        
        Args:
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏ –Ω–∞ HF –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å
            
        Returns:
            –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
        if self.model_cache.is_model_cached("tokenizers", model_name):
            cache_dir = self.model_cache.get_model_cache_dir("tokenizers", model_name)
            tokenizer_path = cache_dir / "tokenizer.json"
            logger.info(f"Loading tokenizer from cache: {tokenizer_path}")
            return Tokenizer.from_file(str(tokenizer_path))
        
        # –°–∫–∞—á–∏–≤–∞–µ–º —Å HuggingFace Hub
        logger.info(f"Downloading tokenizer '{model_name}' from HuggingFace Hub...")
        try:
            cache_dir = self.model_cache.get_model_cache_dir("tokenizers", model_name)
            
            # –°–∫–∞—á–∏–≤–∞–µ–º tokenizer.json
            tokenizer_file = hf_hub_download(
                repo_id=model_name,
                filename="tokenizer.json",
                cache_dir=str(cache_dir),
                local_dir=str(cache_dir),
                local_dir_use_symlinks=False,
            )
            
            tokenizer = Tokenizer.from_file(tokenizer_file)
            logger.info(f"Tokenizer '{model_name}' downloaded and cached successfully")
            return tokenizer
        
        except Exception as e:
            raise RuntimeError(
                f"Failed to load tokenizer '{model_name}' from HuggingFace Hub. "
                f"Ensure the model name is correct and you have internet connection."
            ) from e
    
    def count_tokens(self, text: str) -> int:
        if not text:
            return 0
        encoding = self._tokenizer.encode(text)
        return len(encoding.ids)
    
    def encode(self, text: str) -> List[int]:
        return self._tokenizer.encode(text).ids
    
    def decode(self, token_ids: List[int]) -> str:
        return self._tokenizer.decode(token_ids)
    
    @staticmethod
    def list_available_encoders(root: Path) -> List[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤.
        
        –í–∫–ª—é—á–∞–µ—Ç:
        - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏
        - –£–∂–µ —Å–∫–∞—á–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
        
        Args:
            root: –ö–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∏–º–µ–Ω –º–æ–¥–µ–ª–µ–π
        """
        model_cache = ModelCache(root)
        cached = model_cache.list_cached_models("tokenizers")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∏ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ (–±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)
        all_models = list(RECOMMENDED_TOKENIZERS)
        for cached_model in cached:
            if cached_model not in all_models:
                all_models.append(cached_model)
        
        return all_models
```

---

### 4.5. –ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è SentencePiece

**–§–∞–π–ª**: `lg/stats/tokenizers/sp_adapter.py`

```python
from pathlib import Path
from typing import List
import logging

import sentencepiece as spm
from huggingface_hub import hf_hub_download

from .base import BaseTokenizer
from .model_cache import ModelCache

logger = logging.getLogger(__name__)

# –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ SentencePiece
RECOMMENDED_MODELS = [
    "google/gemma-2-2b",       # Gemma —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (Google)
    "meta-llama/Llama-2-7b-hf", # Llama 2 —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
]

class SPAdapter(BaseTokenizer):
    """–ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ SentencePiece."""
    
    def __init__(self, encoder: str, ctx_limit: int, root: Path):
        super().__init__(encoder, ctx_limit)
        self.root = root
        self.model_cache = ModelCache(root)
        
        self._sp = spm.SentencePieceProcessor()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        model_path = self._load_model(encoder)
        self._sp.load(str(model_path))
    
    def _load_model(self, model_spec: str) -> Path:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç SentencePiece –º–æ–¥–µ–ª—å.
        
        Args:
            model_spec: –ú–æ–∂–µ—Ç –±—ã—Ç—å:
                - –ü—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–º—É .model —Ñ–∞–π–ª—É: /path/to/model.spm
                - –ò–º—è –º–æ–¥–µ–ª–∏ –Ω–∞ HF: google/gemma-2-2b
        
        Returns:
            –ü—É—Ç—å –∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        # –õ–æ–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª
        local_path = Path(model_spec)
        if local_path.exists() and local_path.suffix in [".model", ".spm"]:
            logger.info(f"Loading SentencePiece model from local file: {local_path}")
            return local_path
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
        if self.model_cache.is_model_cached("sentencepiece", model_spec):
            cache_dir = self.model_cache.get_model_cache_dir("sentencepiece", model_spec)
            # –ò—â–µ–º .model —Ñ–∞–π–ª
            model_files = list(cache_dir.glob("*.model"))
            if model_files:
                logger.info(f"Loading SentencePiece model from cache: {model_files[0]}")
                return model_files[0]
        
        # –°–∫–∞—á–∏–≤–∞–µ–º —Å HuggingFace Hub
        logger.info(f"Downloading SentencePiece model '{model_spec}' from HuggingFace Hub...")
        try:
            cache_dir = self.model_cache.get_model_cache_dir("sentencepiece", model_spec)
            
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤
            for filename in ["tokenizer.model", "spiece.model", "sentencepiece.model"]:
                try:
                    model_file = hf_hub_download(
                        repo_id=model_spec,
                        filename=filename,
                        cache_dir=str(cache_dir),
                        local_dir=str(cache_dir),
                        local_dir_use_symlinks=False,
                    )
                    logger.info(f"SentencePiece model '{model_spec}' downloaded and cached successfully")
                    return Path(model_file)
                except Exception:
                    continue
            
            raise FileNotFoundError(
                f"Could not find SentencePiece model file in repository '{model_spec}'. "
                f"Tried: tokenizer.model, spiece.model, sentencepiece.model"
            )
        
        except Exception as e:
            raise RuntimeError(
                f"Failed to load SentencePiece model '{model_spec}'. "
                f"Ensure the model name is correct, it contains a .model file, "
                f"and you have internet connection."
            ) from e
    
    def count_tokens(self, text: str) -> int:
        if not text:
            return 0
        return len(self._sp.encode(text))
    
    def encode(self, text: str) -> List[int]:
        return self._sp.encode(text)
    
    def decode(self, token_ids: List[int]) -> str:
        return self._sp.decode(token_ids)
    
    @staticmethod
    def list_available_encoders(root: Path) -> List[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö SentencePiece –º–æ–¥–µ–ª–µ–π.
        
        –í–∫–ª—é—á–∞–µ—Ç:
        - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏
        - –£–∂–µ —Å–∫–∞—á–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
        - –ü–æ–¥—Å–∫–∞–∑–∫—É –ø—Ä–æ –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã
        
        Args:
            root: –ö–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∏–º–µ–Ω –º–æ–¥–µ–ª–µ–π –∏ –ø–æ–¥—Å–∫–∞–∑–æ–∫
        """
        model_cache = ModelCache(root)
        cached = model_cache.list_cached_models("sentencepiece")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∏ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
        all_models = list(RECOMMENDED_MODELS)
        for cached_model in cached:
            if cached_model not in all_models:
                all_models.append(cached_model)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥—Å–∫–∞–∑–∫—É –ø—Ä–æ –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã
        all_models.append("(or specify local file: /path/to/model.spm)")
        
        return all_models
```

---

### 4.6. –§–∞–±—Ä–∏–∫–∞ –∏ –ø—É–±–ª–∏—á–Ω—ã–π API

**–§–∞–π–ª**: `lg/stats/tokenizers/__init__.py`

```python
from pathlib import Path
from typing import List

from .base import BaseTokenizer
from .tiktoken_adapter import TiktokenAdapter
from .hf_adapter import HFAdapter
from .sp_adapter import SPAdapter

def create_tokenizer(lib: str, encoder: str, ctx_limit: int, root: Path) -> BaseTokenizer:
    """
    –°–æ–∑–¥–∞–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º.
    
    Args:
        lib: –ò–º—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ (tiktoken, tokenizers, sentencepiece)
        encoder: –ò–º—è —ç–Ω–∫–æ–¥–µ—Ä–∞/–º–æ–¥–µ–ª–∏
        ctx_limit: –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö
        root: –ö–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞
        
    Returns:
        –ò–Ω—Å—Ç–∞–Ω—Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        
    Raises:
        ValueError: –ï—Å–ª–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞
    """
    if lib == "tiktoken":
        return TiktokenAdapter(encoder, ctx_limit)
    elif lib == "tokenizers":
        return HFAdapter(encoder, ctx_limit, root)
    elif lib == "sentencepiece":
        return SPAdapter(encoder, ctx_limit, root)
    else:
        raise ValueError(
            f"Unknown tokenizer library: '{lib}'. "
            f"Supported: tiktoken, tokenizers, sentencepiece"
        )

def list_tokenizer_libs() -> List[str]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏."""
    return ["tiktoken", "tokenizers", "sentencepiece"]

def list_encoders(lib: str, root: Path) -> List[str]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –¥–ª—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏.
    
    Args:
        lib: –ò–º—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
        root: –ö–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ (–¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –∫–µ—à—É)
        
    Returns:
        –°–ø–∏—Å–æ–∫ –∏–º–µ–Ω —ç–Ω–∫–æ–¥–µ—Ä–æ–≤/–º–æ–¥–µ–ª–µ–π
        
    Raises:
        ValueError: –ï—Å–ª–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞
    """
    if lib == "tiktoken":
        return TiktokenAdapter.list_available_encoders()
    elif lib == "tokenizers":
        return HFAdapter.list_available_encoders(root)
    elif lib == "sentencepiece":
        return SPAdapter.list_available_encoders(root)
    else:
        raise ValueError(
            f"Unknown tokenizer library: '{lib}'. "
            f"Supported: tiktoken, tokenizers, sentencepiece"
        )

__all__ = [
    "BaseTokenizer",
    "create_tokenizer",
    "list_tokenizer_libs",
    "list_encoders",
]
```

---

### 4.7. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ lg/stats/__init__.py

**–§–∞–π–ª**: `lg/stats/__init__.py`

```python
# –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —ç–∫—Å–ø–æ—Ä—Ç—ã
# from .load import load_models, list_models, get_model_info  ‚ùå –£–î–ê–õ–ò–¢–¨
# from .model import ModelInfo, ModelsConfig, PlanInfo, ResolvedModel  ‚ùå –£–î–ê–õ–ò–¢–¨

# –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —ç–∫—Å–ø–æ—Ä—Ç—ã
from .tokenizers import (
    BaseTokenizer,
    create_tokenizer,
    list_tokenizer_libs,
    list_encoders,
)

# –û—Å—Ç–∞–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —ç–∫—Å–ø–æ—Ä—Ç—ã
from .collector import StatsCollector
from .report_builder import build_run_result_from_collector
from .report_schema import RunResult, Total, File, Context, Scope

__all__ = [
    # Tokenizers
    "BaseTokenizer",
    "create_tokenizer",
    "list_tokenizer_libs",
    "list_encoders",
    
    # Stats
    "StatsCollector",
    "build_run_result_from_collector",
    
    # Report schema
    "RunResult",
    "Total",
    "File",
    "Context",
    "Scope",
]
```

---

### 4.8. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ JSON —Å—Ö–µ–º—ã –æ—Ç—á–µ—Ç–∞

**–§–∞–π–ª**: `lg/stats/report_schema.py`

**–ò–∑–º–µ–Ω–µ–Ω–∏—è**:

```python
# –ë–´–õ–û (protocol 4)
class RunResult(BaseModel):
    protocol: conint(ge=1)
    scope: Scope
    target: str
    model: str              # ‚ùå –£–î–ê–õ–ò–¢–¨
    encoder: str
    ctxLimit: conint(ge=1)
    # ...

# –°–¢–ê–õ–û (protocol 5)
class RunResult(BaseModel):
    protocol: conint(ge=1)
    scope: Scope
    target: str
    tokenizerLib: str       # ‚≠ê –ù–û–í–û–ï: tiktoken|tokenizers|sentencepiece
    encoder: str
    ctxLimit: conint(ge=1)
    # ...
```

**–ü–æ–ª–Ω—ã–π –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª**:

```python
# generated by datamodel-codegen:
#   filename:  report.schema.json

from __future__ import annotations

from enum import Enum
from typing import Optional, Union

from pydantic import BaseModel, ConfigDict, conint


class Scope(Enum):
    context = 'context'
    section = 'section'


class Total(BaseModel):
    model_config = ConfigDict(
        extra='forbid',
    )
    sizeBytes: conint(ge=0)
    tokensProcessed: conint(ge=0)
    tokensRaw: conint(ge=0)
    savedTokens: conint(ge=0)
    savedPct: float
    ctxShare: float
    renderedTokens: Optional[conint(ge=0)] = None
    renderedOverheadTokens: Optional[conint(ge=0)] = None
    metaSummary: dict[str, int]


class File(BaseModel):
    model_config = ConfigDict(
        extra='forbid',
    )
    path: str
    sizeBytes: conint(ge=0)
    tokensRaw: conint(ge=0)
    tokensProcessed: conint(ge=0)
    savedTokens: conint(ge=0)
    savedPct: float
    promptShare: float
    ctxShare: float
    meta: dict[str, Union[str, int, float, bool]]


class Context(BaseModel):
    model_config = ConfigDict(
        extra='forbid',
    )
    templateName: str
    sectionsUsed: dict[str, conint(ge=1)]
    finalRenderedTokens: Optional[conint(ge=0)] = None
    templateOnlyTokens: Optional[conint(ge=0)] = None
    templateOverheadPct: Optional[float] = None
    finalCtxShare: Optional[float] = None


class RunResult(BaseModel):
    model_config = ConfigDict(
        extra='forbid',
    )
    protocol: conint(ge=1)
    scope: Scope
    target: str
    tokenizerLib: str  # ‚≠ê –ù–û–í–û–ï: tiktoken|tokenizers|sentencepiece
    encoder: str
    ctxLimit: conint(ge=1)
    total: Total
    files: list[File]
    context: Optional[Context] = None
```

---

### 4.9. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ lg/stats/collector.py

**–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è**: –∑–∞–º–µ–Ω–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ `TokenService.model_info` –Ω–∞ –ø—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞.

**–ü–∞—Ç—á**:

```python
# –ë–´–õ–û
model_info = self.tokenizer.model_info
prompt_share = (file_stats.tokens_processed / model_info.ctx_limit * 100.0) if model_info.ctx_limit else 0.0

# –°–¢–ê–õ–û
ctx_limit = self.tokenizer.ctx_limit
prompt_share = (file_stats.tokens_processed / ctx_limit * 100.0) if ctx_limit else 0.0
```

**–ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ** –¥–ª—è –≤—Å–µ—Ö –º–µ—Å—Ç, –≥–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è `model_info.ctx_limit` - –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ `self.tokenizer.ctx_limit`.

---

### 4.10. –ü–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∞ lg/stats/tokenizer.py

**–°—Ç–∞—Ä–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è** (`TokenService`) —Å–ª–æ–∂–Ω–∞—è –∏ –ø—Ä–∏–≤—è–∑–∞–Ω–∞ –∫ `models.yaml`. 

**–ù–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è**: —Ç–æ–Ω–∫–∞—è –æ–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ `BaseTokenizer` —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º.

**–§–∞–π–ª**: `lg/stats/tokenizer.py`

```python
from __future__ import annotations

from pathlib import Path
from typing import Tuple, Optional

from .tokenizers import BaseTokenizer, create_tokenizer

"""
–°–µ—Ä–≤–∏—Å –ø–æ–¥—Å—á—ë—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è).

–°–æ–∑–¥–∞—ë—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑ –Ω–∞ —Å—Ç–∞—Ä—Ç–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç
—É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ API –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º–∏.
"""

class TokenService:
    """
    –û–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ BaseTokenizer —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
    """

    def __init__(
        self,
        root: Path,
        lib: str,
        encoder: str,
        ctx_limit: int,
        *,
        cache=None
    ):
        """
        Args:
            root: –ö–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞
            lib: –ò–º—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ (tiktoken, tokenizers, sentencepiece)
            encoder: –ò–º—è —ç–Ω–∫–æ–¥–µ—Ä–∞/–º–æ–¥–µ–ª–∏
            ctx_limit: –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö
            cache: –ö–µ—à –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        self.root = root
        self.lib = lib
        self.encoder = encoder
        self.ctx_limit = ctx_limit
        self.cache = cache
        
        # –°–æ–∑–¥–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        self._tokenizer = create_tokenizer(lib, encoder, ctx_limit, root)

    @property
    def tokenizer(self) -> BaseTokenizer:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–∞–∑–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä."""
        return self._tokenizer

    @property
    def encoder_name(self) -> str:
        """–ò–º—è —ç–Ω–∫–æ–¥–µ—Ä–∞."""
        return self.encoder

    def count_text(self, text: str) -> int:
        """–ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Ç–æ–∫–µ–Ω—ã –≤ —Ç–µ–∫—Å—Ç–µ."""
        return self._tokenizer.count_tokens(text)
    
    def count_text_cached(self, text: str) -> int:
        """
        –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Ç–æ–∫–µ–Ω—ã –≤ —Ç–µ–∫—Å—Ç–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–µ—à–∞.
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤
            
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
        """
        if not text:
            return 0
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –∫–µ—à–∞, –ø—Ä–æ—Å—Ç–æ —Å—á–∏—Ç–∞–µ–º
        if not self.cache:
            return self.count_text(text)
        
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –∫–µ—à–∞
        # –ö–ª—é—á: lib:encoder
        cache_key = f"{self.lib}:{self.encoder}"
        cached_tokens = self.cache.get_text_tokens(text, cache_key)
        if cached_tokens is not None:
            return cached_tokens
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –≤ –∫–µ—à–µ, –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
        token_count = self.count_text(text)
        self.cache.put_text_tokens(text, cache_key, token_count)
        
        return token_count

    def compare_texts(self, original: str, replacement: str) -> Tuple[int, int, int, float]:
        """
        –°—Ä–∞–≤–Ω–∏—Ç—å —Å—Ç–æ–∏–º–æ—Å—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª–∞ –∏ –∑–∞–º–µ–Ω—ã.

        Returns: (orig_tokens, repl_tokens, savings, ratio)
        ratio = savings / max(repl_tokens, 1)
        """
        orig = self.count_text(original)
        repl = self.count_text(replacement)
        savings = max(0, orig - repl)
        ratio = savings / float(max(repl, 1))
        return orig, repl, savings, ratio

    def is_economical(
        self, 
        original: str, 
        replacement: str, 
        *, 
        min_ratio: float, 
        replacement_is_none: bool,
        min_abs_savings_if_none: int
    ) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–µ—Å–æ–æ–±—Ä–∞–∑–Ω–æ—Å—Ç–∏ –∑–∞–º–µ–Ω—ã.

        - –î–ª—è –æ–±—ã—á–Ω—ã—Ö –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–æ–≤ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–æ—Ä–æ–≥ –æ—Ç–Ω–æ—à–µ–Ω–∏—è savings/replacement ‚â• min_ratio.
        - –î–ª—è "–ø—É—Å—Ç—ã—Ö" –∑–∞–º–µ–Ω (replacement_is_none=True) –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –º–æ–∂–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø–æ—Ä–æ–≥
          —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ (min_abs_savings_if_none), —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –º–∏–∫—Ä–æ—Å–∫–æ–ø–∏—á–µ—Å–∫–∏—Ö —É–¥–∞–ª–µ–Ω–∏–π.
        """
        orig, repl, savings, ratio = self.compare_texts(original, replacement)

        if replacement_is_none and savings < min_abs_savings_if_none:
            return False

        return ratio >= float(min_ratio)

    def truncate_to_tokens(self, text: str, max_tokens: int) -> str:
        """
        –£—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ.
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —É—Ä–µ–∑–∞–Ω–∏—è
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            
        Returns:
            –£—Ä–µ–∑–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤
        """
        if not text:
            return ""
        
        current_tokens = self.count_text(text)
        if current_tokens <= max_tokens:
            return text
        
        # –ü—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —É—Ä–µ–∑–∞–Ω–∏–µ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        ratio = max_tokens / current_tokens
        target_length = int(len(text) * ratio)
        
        # –£—Ä–µ–∑–∞–µ–º –¥–æ —Ü–µ–ª–µ–≤–æ–π –¥–ª–∏–Ω—ã, –Ω–æ –Ω–µ –º–µ–Ω—å—à–µ 1 —Å–∏–º–≤–æ–ª–∞
        target_length = max(1, target_length)
        trimmed = text[:target_length].rstrip()
        
        return trimmed
```

---

### 4.11. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ lg/run_context.py

**–ü–∞—Ç—á**: –∑–∞–º–µ–Ω–∏—Ç—å —Å–æ–∑–¥–∞–Ω–∏–µ `TokenService`.

```python
# –ë–´–õ–û
self.tokenizer = TokenService(self.root, self.options.model, cache=self.cache)

# –°–¢–ê–õ–û
self.tokenizer = TokenService(
    root=self.root,
    lib=self.options.tokenizer_lib,
    encoder=self.options.encoder,
    ctx_limit=self.options.ctx_limit,
    cache=self.cache
)
```

---

### 4.12. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ lg/types.py

**–ü–∞—Ç—á**: –æ–±–Ω–æ–≤–∏—Ç—å `RunOptions`.

```python
@dataclass(frozen=True)
class RunOptions:
    # ‚ùå –£–î–ê–õ–ò–¢–¨
    # model: ModelName = ModelName("o3")
    
    # ‚≠ê –ù–û–í–û–ï: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
    tokenizer_lib: str = "tiktoken"
    encoder: str = "cl100k_base"
    ctx_limit: int = 128000
    
    # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
    modes: Dict[str, str] = field(default_factory=dict)
    extra_tags: Set[str] = field(default_factory=set)
    task_text: Optional[str] = None
    target_branch: Optional[str] = None
```

---

### 4.13. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ lg/cli.py

**–ë–æ–ª—å—à–æ–π –ø–∞—Ç—á**: –æ–±–Ω–æ–≤–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥ –∏ –¥–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–µ –ø–æ–¥–∫–æ–º–∞–Ω–¥—ã.

#### –£–¥–∞–ª–∏—Ç—å --model –∏–∑ render/report

```python
def add_common(sp: argparse.ArgumentParser) -> None:
    sp.add_argument(
        "target",
        help="ctx:<name> | sec:<name> | <name> (—Å–Ω–∞—á–∞–ª–∞ –∏—â–µ—Ç—Å—è –∫–æ–Ω—Ç–µ–∫—Å—Ç, –∏–Ω–∞—á–µ —Å–µ–∫—Ü–∏—è)",
    )
    
    # ‚ùå –£–î–ê–õ–ò–¢–¨
    # sp.add_argument("--model", default="o3", help="–±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏")
    
    # ‚≠ê –ù–û–í–û–ï: —è–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
    sp.add_argument(
        "--lib",
        required=True,
        choices=["tiktoken", "tokenizers", "sentencepiece"],
        help="–±–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏"
    )
    sp.add_argument(
        "--encoder",
        required=True,
        help="–∏–º—è —ç–Ω–∫–æ–¥–µ—Ä–∞/–º–æ–¥–µ–ª–∏"
    )
    sp.add_argument(
        "--ctx-limit",
        type=int,
        required=True,
        help="—Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö"
    )
    
    # –û—Å—Ç–∞–ª—å–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
    sp.add_argument("--mode", action="append", metavar="MODESET:MODE", ...)
    sp.add_argument("--tags", help="–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ–≥–∏ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é")
    sp.add_argument("--task", metavar="TEXT|@FILE|-", help=...)
    sp.add_argument("--target-branch", metavar="BRANCH", help=...)
```

#### –û–±–Ω–æ–≤–∏—Ç—å _opts()

```python
def _opts(ns: argparse.Namespace) -> RunOptions:
    modes = _parse_modes(getattr(ns, "mode", None))
    extra_tags = _parse_tags(getattr(ns, "tags", None))
    task_text = _parse_task(getattr(ns, "task", None))
    target_branch = getattr(ns, "target_branch", None)
    
    return RunOptions(
        tokenizer_lib=ns.lib,
        encoder=ns.encoder,
        ctx_limit=ns.ctx_limit,
        modes=modes,
        extra_tags=extra_tags,
        task_text=task_text,
        target_branch=target_branch,
    )
```

#### –£–¥–∞–ª–∏—Ç—å –ø–æ–¥–∫–æ–º–∞–Ω–¥—É list models

```python
# ‚ùå –£–î–ê–õ–ò–¢–¨ –¶–ï–õ–´–ô –ë–õ–û–ö
# if ns.what == "models":
#     from .stats import list_models
#     data = {"models": list_models(root)}
```

#### –î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–µ –ø–æ–¥–∫–æ–º–∞–Ω–¥—ã

```python
# –í sp_list (–ø–æ—Å–ª–µ "tag-sets")
sp_list = sub.add_parser("list", help="–°–ø–∏—Å–∫–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π (JSON)")
sp_list.add_argument(
    "what",
    choices=[
        "contexts",
        "sections",
        "mode-sets",
        "tag-sets",
        "tokenizer-libs",  # ‚≠ê –ù–û–í–û–ï
        "encoders"         # ‚≠ê –ù–û–í–û–ï
    ],
    help="—á—Ç–æ –≤—ã–≤–µ—Å—Ç–∏"
)

# –î–ª—è encoders –Ω—É–∂–µ–Ω --lib
sp_list.add_argument(
    "--lib",
    choices=["tiktoken", "tokenizers", "sentencepiece"],
    help="–±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —Å–ø–∏—Å–∫–∞ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ (—Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–ª—è what=encoders)"
)

# –í main() –¥–æ–±–∞–≤–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É
if ns.what == "tokenizer-libs":
    from .stats import list_tokenizer_libs
    data = {"tokenizer_libs": list_tokenizer_libs()}

elif ns.what == "encoders":
    if not ns.lib:
        sys.stderr.write("Error: --lib is required for 'encoders'\n")
        return 2
    from .stats import list_encoders
    root = Path.cwd()
    data = {"lib": ns.lib, "encoders": list_encoders(ns.lib, root)}
```

---

### 4.14. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ lg/stats/report_builder.py

**–ü–∞—Ç—á**: —É–ø—Ä–æ—Å—Ç–∏—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –æ—Ç—á–µ—Ç–∞ (—É–±—Ä–∞—Ç—å –ª–æ–≥–∏–∫—É –ø–ª–∞–Ω–æ–≤).

```python
def build_run_result_from_collector(
    collector: StatsCollector,
    target_spec: TargetSpec
) -> RunResult:
    """
    –°—Ç—Ä–æ–∏—Ç RunResult –∏–∑ —Å–æ–±—Ä–∞–Ω–Ω–æ–π –∫–æ–ª–ª–µ–∫—Ç–æ—Ä–æ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏.
    """
    files_rows, totals, ctx_block = collector.compute_final_stats()
    
    # ‚ùå –£–î–ê–õ–ò–¢–¨
    # model_info = collector.tokenizer.model_info
    
    # ‚≠ê –ù–û–í–û–ï: –±–µ—Ä–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞–ø—Ä—è–º—É—é
    tokenizer = collector.tokenizer
    
    # –ú—ç–ø–ø–∏–Ω–≥ Totals –≤ Total
    total = Total(
        sizeBytes=totals.sizeBytes,
        tokensProcessed=totals.tokensProcessed,
        tokensRaw=totals.tokensRaw,
        savedTokens=totals.savedTokens,
        savedPct=totals.savedPct,
        ctxShare=totals.ctxShare,
        renderedTokens=totals.renderedTokens,
        renderedOverheadTokens=totals.renderedOverheadTokens,
        metaSummary=dict(totals.metaSummary or {}),
    )

    # –ú—ç–ø–ø–∏–Ω–≥ —Ñ–∞–π–ª–æ–≤
    files = [
        File(
            path=row.path,
            sizeBytes=row.sizeBytes,
            tokensRaw=row.tokensRaw,
            tokensProcessed=row.tokensProcessed,
            savedTokens=row.savedTokens,
            savedPct=row.savedPct,
            promptShare=row.promptShare,
            ctxShare=row.ctxShare,
            meta=dict(row.meta or {}),
        )
        for row in files_rows
    ]

    # Scope –∏ target
    scope = Scope.context if target_spec.kind == "context" else Scope.section
    target_norm = f"{'ctx' if target_spec.kind == 'context' else 'sec'}:{target_spec.name}"

    # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –±–ª–æ–∫
    context: Context | None = None
    if scope is Scope.context:
        context = Context(
            templateName=ctx_block.templateName,
            sectionsUsed=dict(ctx_block.sectionsUsed),
            finalRenderedTokens=ctx_block.finalRenderedTokens,
            templateOnlyTokens=ctx_block.templateOnlyTokens,
            templateOverheadPct=ctx_block.templateOverheadPct,
            finalCtxShare=ctx_block.finalCtxShare,
        )

    # ‚≠ê –ù–û–í–û–ï: protocol 5 —Å –Ω–æ–≤—ã–º–∏ –ø–æ–ª—è–º–∏
    result = RunResult(
        protocol=5,  # Bump –≤–µ—Ä—Å–∏–∏
        scope=scope,
        target=target_norm,
        tokenizerLib=tokenizer.lib,
        encoder=tokenizer.encoder,
        ctxLimit=tokenizer.ctx_limit,
        total=total,
        files=files,
        context=context,
    )
    
    return result
```

---

### 4.15. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ lg/protocol.py

**–§–∞–π–ª**: `lg/protocol.py`

```python
# –ë–´–õ–û
PROTOCOL_VERSION = 4

# –°–¢–ê–õ–û
PROTOCOL_VERSION = 5
```

---

### 4.16. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ pyproject.toml

**–§–∞–π–ª**: `pyproject.toml`

```toml
[project]
dependencies = [
    "ruamel.yaml>=0.18",
    "pathspec>=0.12",
    "tiktoken>=0.6",
    "tokenizers>=0.15",         # ‚≠ê –ù–û–í–û–ï
    "sentencepiece>=0.2",       # ‚≠ê –ù–û–í–û–ï
    "huggingface-hub>=0.20",    # ‚≠ê –ù–û–í–û–ï
    "pydantic>=2.0,<3.0",
    "tree-sitter>=0.21",
    "tree-sitter-python>=0.23",
    "tree-sitter-typescript>=0.23",
    "tree-sitter-javascript>=0.25",
]
```

---

## 5. –ú–∏–≥—Ä–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö

### 5.1. –ß—Ç–æ —É–¥–∞–ª–∏—Ç—å —á–µ—Ä–µ–∑ –º–∏–≥—Ä–∞—Ü–∏—é

**–£–¥–∞–ª—è–µ–º—ã–µ —Ñ–∞–π–ª—ã –∏–∑ lg-cfg/**:
- `models.yaml` - –ø–æ–ª–Ω–æ—Å—Ç—å—é —É—Å—Ç–∞—Ä–µ–≤—à–∏–π —Ñ–æ—Ä–º–∞—Ç

### 5.2. –ß—Ç–æ –¥–æ–±–∞–≤–∏—Ç—å —á–µ—Ä–µ–∑ –º–∏–≥—Ä–∞—Ü–∏—é

**–ù–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏ –≤ .gitignore**:
```gitignore
# lg-cfg/.gitignore
tokenizer-models/
```

### 5.3. –°–∫—Ä–∏–ø—Ç –º–∏–≥—Ä–∞—Ü–∏–∏

**–§–∞–π–ª**: `lg/migrate/migrations/m006_remove_models_yaml.py`

```python
"""
M006: –£–¥–∞–ª–µ–Ω–∏–µ —É—Å—Ç–∞—Ä–µ–≤—à–µ–≥–æ models.yaml.

–£–¥–∞–ª—è–µ—Ç lg-cfg/models.yaml, —Ç–∞–∫ –∫–∞–∫ –Ω–æ–≤–∞—è –≤–µ—Ä—Å–∏—è LG –∏—Å–ø–æ–ª—å–∑—É–µ—Ç
—è–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ CLI.
"""

from pathlib import Path
from typing import List

from ..errors import MigrationError
from ..model import Migration


def migrate(cfg_root: Path) -> List[str]:
    """
    –£–¥–∞–ª—è–µ—Ç models.yaml –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç tokenizer-models/ –≤ .gitignore.
    
    Args:
        cfg_root: –ü—É—Ç—å –∫ lg-cfg/
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏—è—Ö
    """
    actions = []
    
    # –£–¥–∞–ª—è–µ–º models.yaml –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    models_path = cfg_root / "models.yaml"
    if models_path.exists():
        models_path.unlink()
        actions.append("Removed obsolete models.yaml")
    
    # –î–æ–±–∞–≤–ª—è–µ–º tokenizer-models/ –≤ .gitignore
    gitignore_path = cfg_root / ".gitignore"
    entry = "tokenizer-models/\n"
    
    if gitignore_path.exists():
        content = gitignore_path.read_text(encoding="utf-8")
        if "tokenizer-models" not in content:
            gitignore_path.write_text(content + entry, encoding="utf-8")
            actions.append("Added tokenizer-models/ to .gitignore")
    else:
        gitignore_path.write_text(entry, encoding="utf-8")
        actions.append("Created .gitignore with tokenizer-models/")
    
    return actions


# –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –º–∏–≥—Ä–∞—Ü–∏–∏
M006 = Migration(
    id="m006_remove_models_yaml",
    description="Remove obsolete models.yaml and prepare for new tokenization system",
    apply=migrate,
)
```

**–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –≤ —Å–ø–∏—Å–∫–µ –º–∏–≥—Ä–∞—Ü–∏–π**:

```python
# lg/migrate/registry.py

from .migrations.m006_remove_models_yaml import M006

MIGRATIONS = [
    # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–∏–≥—Ä–∞—Ü–∏–∏ ...
    M006,
]
```

---

## 6. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### 6.1. –†—É—á–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```bash
# 1. –°–ø–∏—Å–æ–∫ –±–∏–±–ª–∏–æ—Ç–µ–∫
lg list tokenizer-libs
# Expected: ["tiktoken", "tokenizers", "sentencepiece"]

# 2. –°–ø–∏—Å–æ–∫ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –¥–ª—è tiktoken
lg list encoders --lib tiktoken
# Expected: ["gpt2", "r50k_base", "p50k_base", "cl100k_base", "o200k_base"]

# 3. –°–ø–∏—Å–æ–∫ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –¥–ª—è tokenizers (–ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ - —Ç–æ–ª—å–∫–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ)
lg list encoders --lib tokenizers
# Expected: ["gpt2", "roberta-base", "bert-base-uncased", ...]

# 4. –†–µ–Ω–¥–µ—Ä–∏–Ω–≥ —Å tiktoken
lg render ctx:all --lib tiktoken --encoder cl100k_base --ctx-limit 128000

# 5. –û—Ç—á–µ—Ç —Å tokenizers (–∞–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏)
lg report sec:core --lib tokenizers --encoder gpt2 --ctx-limit 50000 > report.json

# 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ JSON —Å—Ö–µ–º—ã
cat report.json | jq '.protocol, .tokenizerLib, .encoder, .ctxLimit'
# Expected: 5, "tokenizers", "gpt2", 50000

# 7. –°–ø–∏—Å–æ–∫ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –ø–æ—Å–ª–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
lg list encoders --lib tokenizers
# Expected: –≤–∫–ª—é—á–∞–µ—Ç "gpt2" –∫–∞–∫ —É–∂–µ —Å–∫–∞—á–∞–Ω–Ω—ã–π

# 8. SentencePiece —Å HF –º–æ–¥–µ–ª—å—é
lg render ctx:all --lib sentencepiece --encoder google/gemma-2-2b --ctx-limit 1000000

# 9. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–µ—à–∞
ls -la lg-cfg/tokenizer-models/tokenizers/
ls -la lg-cfg/tokenizer-models/sentencepiece/
```

### 6.2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—à–∏–±–æ–∫

```bash
# –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞
lg list encoders --lib unknown
# Expected: ValueError —Å —Å–æ–æ–±—â–µ–Ω–∏–µ–º

# –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç --lib –¥–ª—è encoders
lg list encoders
# Expected: Error message

# –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä tiktoken
lg render ctx:all --lib tiktoken --encoder unknown --ctx-limit 128000
# Expected: ValueError —Å–æ —Å–ø–∏—Å–∫–æ–º –¥–æ—Å—Ç—É–ø–Ω—ã—Ö

# –ù–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è HF –º–æ–¥–µ–ª—å
lg render ctx:all --lib tokenizers --encoder fake/model --ctx-limit 128000
# Expected: RuntimeError —Å –ø–æ–¥—Å–∫–∞–∑–∫–æ–π
```

---

## 7. –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

### 7.1. –û–±–Ω–æ–≤–∏—Ç—å README.md

**–î–æ–±–∞–≤–∏—Ç—å —Ä–∞–∑–¥–µ–ª "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"**:

````markdown
## –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

LG –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –±–∏–±–ª–∏–æ—Ç–µ–∫ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —Ç–æ–∫–µ–Ω–∞–º:

- **tiktoken** (OpenAI) - –¥–ª—è GPT-–º–æ–¥–µ–ª–µ–π
- **tokenizers** (HuggingFace) - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
- **sentencepiece** (Google) - –¥–ª—è Gemini –∏ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º–æ–¥–µ–ª–µ–π

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

–ü—Ä–∏ –≤—ã–∑–æ–≤–µ `render` –∏–ª–∏ `report` —É–∫–∞–∂–∏—Ç–µ —Ç—Ä–∏ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞:

```bash
lg report ctx:all \
  --lib tiktoken \
  --encoder cl100k_base \
  --ctx-limit 128000
```

### –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫

```bash
lg list tokenizer-libs
```

### –°–ø–∏—Å–æ–∫ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –¥–ª—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏

```bash
lg list encoders --lib tiktoken
lg list encoders --lib tokenizers
lg list encoders --lib sentencepiece
```

–ü—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏ –∏–∑ HuggingFace –±—É–¥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∫–∞—á–∞–Ω—ã –∏ –∑–∞–∫–µ—à–∏—Ä–æ–≤–∞–Ω—ã –≤ `lg-cfg/tokenizer-models/`.

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É

| –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ... | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|---------------------|--------------|
| GPT-4, GPT-3.5 | `--lib tiktoken --encoder cl100k_base` |
| GPT-4o, o1, o3 | `--lib tiktoken --encoder o200k_base` |
| Claude 3.5 | `--lib sentencepiece --encoder google/gemma-2-2b` (–ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ) |
| Gemini 2.5 | `--lib sentencepiece --encoder google/gemma-2-2b` |
| Grok | `--lib tokenizers --encoder gpt2` (–ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ) |
| Llama 3 | `--lib sentencepiece --encoder meta-llama/Llama-2-7b-hf` |
````

---

## 8. –ß–µ–∫–ª–∏—Å—Ç —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

### –≠—Ç–∞–ø 1: –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–≥–æ –∫–æ–¥–∞

- [ ] –£–¥–∞–ª–∏—Ç—å `lg/stats/load.py`
- [ ] –£–¥–∞–ª–∏—Ç—å `lg/stats/model.py`
- [ ] –£–¥–∞–ª–∏—Ç—å –∏–∑ `lg/stats/__init__.py` —ç–∫—Å–ø–æ—Ä—Ç—ã: `load_models`, `list_models`, `get_model_info`, `ModelInfo`, `ModelsConfig`, `PlanInfo`, `ResolvedModel`
- [ ] –£–¥–∞–ª–∏—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç `--model` –∏–∑ CLI (`lg/cli.py`)
- [ ] –£–¥–∞–ª–∏—Ç—å –ø–æ–¥–∫–æ–º–∞–Ω–¥—É `lg list models` –∏–∑ CLI
- [ ] –£–¥–∞–ª–∏—Ç—å –ø–æ–ª–µ `model` –∏–∑ `RunOptions` –≤ `lg/types.py`

### –≠—Ç–∞–ø 2: –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏

- [ ] –°–æ–∑–¥–∞—Ç—å `lg/stats/tokenizers/__init__.py`
- [ ] –°–æ–∑–¥–∞—Ç—å `lg/stats/tokenizers/base.py` (BaseTokenizer)
- [ ] –°–æ–∑–¥–∞—Ç—å `lg/stats/tokenizers/model_cache.py` (ModelCache)
- [ ] –°–æ–∑–¥–∞—Ç—å `lg/stats/tokenizers/tiktoken_adapter.py` (TiktokenAdapter)
- [ ] –°–æ–∑–¥–∞—Ç—å `lg/stats/tokenizers/hf_adapter.py` (HFAdapter)
- [ ] –°–æ–∑–¥–∞—Ç—å `lg/stats/tokenizers/sp_adapter.py` (SPAdapter)
- [ ] –û–±–Ω–æ–≤–∏—Ç—å `lg/stats/tokenizers/__init__.py` (—Ñ–∞–±—Ä–∏–∫–∞ –∏ —ç–∫—Å–ø–æ—Ä—Ç—ã)

### –≠—Ç–∞–ø 3: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º

- [ ] –ü–µ—Ä–µ–ø–∏—Å–∞—Ç—å `lg/stats/tokenizer.py` (TokenService –∫–∞–∫ –æ–±—ë—Ä—Ç–∫–∞)
- [ ] –û–±–Ω–æ–≤–∏—Ç—å `lg/stats/__init__.py` (–Ω–æ–≤—ã–µ —ç–∫—Å–ø–æ—Ä—Ç—ã)
- [ ] –û–±–Ω–æ–≤–∏—Ç—å `lg/stats/report_schema.py` (protocol 5, –Ω–æ–≤—ã–µ –ø–æ–ª—è)
- [ ] –û–±–Ω–æ–≤–∏—Ç—å `lg/stats/report_builder.py` (—É–±—Ä–∞—Ç—å –ª–æ–≥–∏–∫—É –ø–ª–∞–Ω–æ–≤)
- [ ] –û–±–Ω–æ–≤–∏—Ç—å `lg/stats/collector.py` (–∑–∞–º–µ–Ω–∏—Ç—å model_info –Ω–∞ –ø—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø)
- [ ] –û–±–Ω–æ–≤–∏—Ç—å `lg/protocol.py` (PROTOCOL_VERSION = 5)
- [ ] –û–±–Ω–æ–≤–∏—Ç—å `lg/types.py` (RunOptions: –¥–æ–±–∞–≤–∏—Ç—å tokenizer_lib/encoder/ctx_limit)
- [ ] –û–±–Ω–æ–≤–∏—Ç—å `lg/run_context.py` (—Å–æ–∑–¥–∞–Ω–∏–µ TokenService —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏)

### –≠—Ç–∞–ø 4: CLI

- [ ] –î–æ–±–∞–≤–∏—Ç—å `--lib`, `--encoder`, `--ctx-limit` –≤ –∫–æ–º–∞–Ω–¥—ã `render`/`report`
- [ ] –û–±–Ω–æ–≤–∏—Ç—å `_opts()` –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è RunOptions —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
- [ ] –î–æ–±–∞–≤–∏—Ç—å `tokenizer-libs` –≤ choices –¥–ª—è `lg list`
- [ ] –î–æ–±–∞–≤–∏—Ç—å `encoders` –≤ choices –¥–ª—è `lg list` —Å –æ–ø—Ü–∏–µ–π `--lib`
- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É `lg list tokenizer-libs`
- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É `lg list encoders --lib <lib>`

### –≠—Ç–∞–ø 5: –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

- [ ] –î–æ–±–∞–≤–∏—Ç—å –≤ `pyproject.toml`: `tokenizers>=0.15`
- [ ] –î–æ–±–∞–≤–∏—Ç—å –≤ `pyproject.toml`: `sentencepiece>=0.2`
- [ ] –î–æ–±–∞–≤–∏—Ç—å –≤ `pyproject.toml`: `huggingface-hub>=0.20`

### –≠—Ç–∞–ø 6: –ú–∏–≥—Ä–∞—Ü–∏—è

- [ ] –°–æ–∑–¥–∞—Ç—å `lg/migrate/migrations/m006_remove_models_yaml.py`
- [ ] –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å M006 –≤ `lg/migrate/registry.py`

### –≠—Ç–∞–ø 7: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

- [ ] –†—É—á–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –Ω–æ–≤—ã—Ö –∫–æ–º–∞–Ω–¥
- [ ] –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π
- [ ] –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
- [ ] –ü—Ä–æ–≤–µ—Ä–∫–∞ JSON —Å—Ö–µ–º—ã (protocol 5)
- [ ] –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫

### –≠—Ç–∞–ø 8: –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

- [ ] –û–±–Ω–æ–≤–∏—Ç—å README.md (—Ä–∞–∑–¥–µ–ª –æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏)
- [ ] –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
- [ ] –î–æ–±–∞–≤–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π

---

## 9. –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è

### –ü—Ä–æ–±–ª–µ–º–∞ 1: –ú–µ–¥–ª–µ–Ω–Ω–∞—è –ø–µ—Ä–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏

**–°–∏–º–ø—Ç–æ–º**: –ü—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ `--lib tokenizers --encoder bert-base-uncased` –¥–æ–ª–≥–æ –≤–∏—Å–∏—Ç –±–µ–∑ –≤—ã–≤–æ–¥–∞.

**–†–µ—à–µ–Ω–∏–µ**: –î–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –∞–¥–∞–ø—Ç–µ—Ä—ã.

```python
# –í hf_adapter.py –∏ sp_adapter.py
logger.info(f"Downloading model '{model_name}' from HuggingFace Hub...")
# –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏:
logger.info(f"Model '{model_name}' downloaded and cached successfully")
```

### –ü—Ä–æ–±–ª–µ–º–∞ 2: –ö–æ–Ω—Ñ–ª–∏–∫—Ç —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–µ—à–µ–º HuggingFace

**–°–∏–º–ø—Ç–æ–º**: –ú–æ–¥–µ–ª–∏ —Å–∫–∞—á–∏–≤–∞—é—Ç—Å—è –≤ `~/.cache/huggingface/` –≤–º–µ—Å—Ç–æ `lg-cfg/tokenizer-models/`.

**–†–µ—à–µ–Ω–∏–µ**: –£–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ –≤ `hf_hub_download` –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:
```python
local_dir=str(cache_dir),
local_dir_use_symlinks=False,
```

### –ü—Ä–æ–±–ª–µ–º–∞ 3: SentencePiece –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ HF —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏

**–°–∏–º–ø—Ç–æ–º**: –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ø—ã—Ç–∫–µ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: "Could not find SentencePiece model file".

**–†–µ—à–µ–Ω–∏–µ**: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª–∏ —Ö—Ä–∞–Ω—è—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –ø–æ–¥ —Ä–∞–∑–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏. –†–∞—Å—à–∏—Ä–∏—Ç—å —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã—Ö –∏–º–µ–Ω –≤ `sp_adapter.py`:

```python
for filename in ["tokenizer.model", "spiece.model", "sentencepiece.model", "sp.model"]:
    # ...
```

### –ü—Ä–æ–±–ª–µ–º–∞ 4: –ù–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –≤–µ—Ä—Å–∏–π –±–∏–±–ª–∏–æ—Ç–µ–∫

**–°–∏–º–ø—Ç–æ–º**: Import errors –∏–ª–∏ runtime errors –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤.

**–†–µ—à–µ–Ω–∏–µ**: –ó–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ –≤ `pyproject.toml` –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ —á–∏—Å—Ç–æ–º –æ–∫—Ä—É–∂–µ–Ω–∏–∏.

---

## 10. –ò—Ç–æ–≥–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (–¥–∏–∞–≥—Ä–∞–º–º–∞)

```
CLI (lg/cli.py)
  ‚îî‚îÄ> parse args: --lib, --encoder, --ctx-limit
       ‚îî‚îÄ> RunOptions
            ‚îî‚îÄ> RunContext
                 ‚îî‚îÄ> TokenService
                      ‚îî‚îÄ> create_tokenizer(lib, encoder, ctx_limit, root)
                           ‚îú‚îÄ> TiktokenAdapter (–≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ encodings)
                           ‚îú‚îÄ> HFAdapter (HF Hub + ModelCache)
                           ‚îî‚îÄ> SPAdapter (HF Hub + ModelCache + –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã)

ModelCache (lg/stats/tokenizers/model_cache.py)
  ‚îî‚îÄ> lg-cfg/tokenizer-models/
       ‚îú‚îÄ> tokenizers/
       ‚îÇ    ‚îú‚îÄ> gpt2/
       ‚îÇ    ‚îú‚îÄ> bert-base-uncased/
       ‚îÇ    ‚îî‚îÄ> google--gemma-tokenizer/
       ‚îî‚îÄ> sentencepiece/
            ‚îú‚îÄ> google--gemma-2-2b/
            ‚îî‚îÄ> meta-llama--Llama-2-7b-hf/

StatsCollector
  ‚îî‚îÄ> count_text_cached(text)
       ‚îî‚îÄ> TokenService.count_text_cached()
            ‚îî‚îÄ> BaseTokenizer.count_tokens()

RunResult (protocol 5)
  ‚îú‚îÄ tokenizerLib: "tiktoken" | "tokenizers" | "sentencepiece"
  ‚îú‚îÄ encoder: "cl100k_base" | "gpt2" | "google/gemma-2-2b"
  ‚îî‚îÄ ctxLimit: 128000
```
