# –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ: –ü–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–¥—Å–∏—Å—Ç–µ–º—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤ LG

## 1. –ë–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

### –ü—Ä–æ–±–ª–µ–º—ã —Ç–µ–∫—É—â–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

1. **–£—Å—Ç–∞—Ä–µ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏**: `lg-cfg/models.yaml` —Å —Ö–∞—Ä–¥–∫–æ–¥-—Å–ø–∏—Å–∫–æ–º –º–æ–¥–µ–ª–µ–π –∏ –ø–ª–∞–Ω–æ–≤ –±—ã—Å—Ç—Ä–æ —É—Å—Ç–∞—Ä–µ–≤–∞–µ—Ç –∏–∑-–∑–∞ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –∏–Ω–¥—É—Å—Ç—Ä–∏–∏ LLM.

2. **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å**: –ü—Ä–∏–≤—è–∑–∫–∞ –∫ `tiktoken` (OpenAI) –Ω–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º–∏ –¥—Ä—É–≥–∏—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ (Anthropic, Google, xAI).

3. **–°–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ª–∏–º–∏—Ç–æ–≤**: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ —á–µ—Ä–µ–∑ "–ø–ª–∞–Ω—ã –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤" –∏–∑–±—ã—Ç–æ—á–Ω–æ –∏ —Ç—Ä—É–¥–Ω–æ –∞–∫—Ç—É–∞–ª–∏–∑–∏—Ä—É–µ–º–æ.

### –¶–µ–ª–µ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ

**–Ø–≤–Ω—ã–π API –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏**: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫—É, —ç–Ω–∫–æ–¥–µ—Ä –∏ —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ —á–µ—Ä–µ–∑ CLI-–∞—Ä–≥—É–º–µ–Ω—Ç—ã –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∑–∞–ø—É—Å–∫–µ. –≠—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:

- **–ü—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å**: –Ω–µ—Ç —Å–∫—Ä—ã—Ç–æ–π –º–∞–≥–∏–∏, –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —è–≤–Ω—ã–µ
- **–ì–∏–±–∫–æ—Å—Ç—å**: –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
- **–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å**: –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ LG –ø—Ä–∏ –ø–æ—è–≤–ª–µ–Ω–∏–∏ –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
- **–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å**: —Ä–∞–±–æ—Ç–∞ —Å –ª—é–±—ã–º–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏ —á–µ—Ä–µ–∑ –æ–ø–µ–Ω—Å–æ—Ä—Å–Ω—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã

### –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

LG –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ —á–µ—Ä–µ–∑ IDE-–∞–¥–¥–æ–Ω—ã (VS Code, JetBrains), –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç:
- –†–∞–∑–≤–∏—Ç—ã–π UI –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
- –°–∏—Å—Ç–µ–º—É –ø—Ä–æ—Ñ–∏–ª–µ–π –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
- –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —á–∞—Å—Ç–æ–≥–æ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –±–µ–∑ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è YAML

–ü–æ—ç—Ç–æ–º—É verbose CLI (3 –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –≤–º–µ—Å—Ç–æ 1) –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–æ–π –¥–ª—è UX.

---

## 2. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

### 2.1. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏

–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º —Ç—Ä–∏ –æ–ø–µ–Ω—Å–æ—Ä—Å–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:

1. **`tiktoken`** (OpenAI) - GPT-–º–æ–¥–µ–ª–∏, –±—ã—Å—Ç—Ä—ã–π C-–∞–¥–∞–ø—Ç–µ—Ä
2. **`tokenizers`** (HuggingFace) - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è Rust-–±–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
3. **`sentencepiece`** (Google) - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ Gemini –∏ –º–Ω–æ–≥–∏—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö –º–æ–¥–µ–ª—è—Ö

**–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ**: —ç—Ç–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –ø–æ–∫—Ä—ã–≤–∞—é—Ç >90% –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –î–ª—è –∑–∞–∫—Ä—ã—Ç—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ (Claude, Grok) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è approximation —á–µ—Ä–µ–∑ –ø–æ—Ö–æ–∂–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã.

### 2.2. –ù–æ–≤–æ–µ CLI

#### –£–¥–∞–ª—è–µ–º—ã–µ –∫–æ–º–∞–Ω–¥—ã

```bash
# –£–î–ê–õ–ò–¢–¨
lg report ctx:all --model gpt-4o
lg list models
```

#### –ù–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã

```bash
# –°–ø–∏—Å–æ–∫ –±–∏–±–ª–∏–æ—Ç–µ–∫
lg list tokenizer-libs
# ‚Üí ["tiktoken", "tokenizers", "sentencepiece"]

# –°–ø–∏—Å–æ–∫ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –¥–ª—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
lg list encoders --lib tiktoken
lg list encoders --lib tokenizers
lg list encoders --lib sentencepiece

# –†–µ–Ω–¥–µ—Ä–∏–Ω–≥/–æ—Ç—á–µ—Ç —Å —è–≤–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (–≤—Å–µ 3 –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã)
lg render ctx:all --lib tiktoken --encoder cl100k_base --ctx-limit 128000
lg report sec:core --lib sentencepiece --encoder google/gemma-2-2b --ctx-limit 1000000
```

### 2.3. –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

- **–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è**: `lg-cfg/tokenizer-models/` (–≤–Ω—É—Ç—Ä–∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è git)
- **–°—Ç—Ä—É–∫—Ç—É—Ä–∞**: `lg-cfg/tokenizer-models/{lib}/{model_name}/`
- **–ê–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–∞**: –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏ —Å–∫–∞—á–∏–≤–∞—é—Ç—Å—è —Å HuggingFace Hub
- **–û—Ç–¥–µ–ª—å–Ω—ã–π –º–æ–¥—É–ª—å**: `lg/stats/model_cache.py` (–Ω–µ —Å–º–µ—à–∏–≤–∞—Ç—å —Å `lg/cache/fs_cache.py`)

### 2.4. –û–±—Ä–∞–±–æ—Ç–∫–∞ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤

#### `tiktoken`
- –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —ç–Ω–∫–æ–¥–µ—Ä—ã: `gpt2`, `r50k_base`, `p50k_base`, `cl100k_base`, `o200k_base`
- –°–ø–∏—Å–æ–∫ —á–µ—Ä–µ–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π API: `tiktoken.list_encoding_names()`
- –ù–µ —Ç—Ä–µ–±—É—é—Ç —Å–∫–∞—á–∏–≤–∞–Ω–∏—è

#### `tokenizers` (HuggingFace)
- –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏ (—Ö–∞—Ä–¥–∫–æ–¥-—Å–ø–∏—Å–æ–∫): `gpt2`, `roberta-base`, `bert-base-uncased`, `bert-base-cased`, `t5-base`, `google/gemma-tokenizer`
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ª—é–±—ã—Ö –º–æ–¥–µ–ª–µ–π —Å HF Hub —á–µ—Ä–µ–∑ `Tokenizer.from_pretrained(model_name)`
- –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ `lg-cfg/tokenizer-models/tokenizers/{model_name}/`

#### `sentencepiece`
- –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏ (—Ö–∞—Ä–¥–∫–æ–¥-—Å–ø–∏—Å–æ–∫): `google/gemma-2-2b`, `meta-llama/Llama-2-7b-hf`
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö `.spm` —Ñ–∞–π–ª–æ–≤: `--encoder /path/to/model.spm`
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ª—é–±—ã—Ö –º–æ–¥–µ–ª–µ–π —Å HF Hub
- –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ `lg-cfg/tokenizer-models/sentencepiece/{model_name}/`

### 2.5. JSON —Å—Ö–µ–º–∞ –æ—Ç—á–µ—Ç–∞ (protocol v5)

```json
{
  "protocol": 5,
  "tokenizerLib": "tiktoken",
  "encoder": "o200k_base",
  "ctxLimit": 200000,
  "scope": "context",
  "target": "ctx:all",
  "total": { ... },
  "files": [ ... ],
  "context": { ... }
}
```

**–ò–∑–º–µ–Ω–µ–Ω–∏—è**:
- `protocol`: 4 ‚Üí 5
- **–£–î–ê–õ–ï–ù–û**: –ø–æ–ª–µ `model`
- **–î–û–ë–ê–í–õ–ï–ù–û**: `tokenizerLib` (—Å—Ç—Ä–æ–∫–∞: `tiktoken|tokenizers|sentencepiece`)
- **–°–û–•–†–ê–ù–ï–ù–û**: `encoder`, `ctxLimit`

---

## 3. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ä–µ—à–µ–Ω–∏—è

### 3.1. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–æ–¥—É–ª—è `lg/stats/`

#### –£–¥–∞–ª—è–µ–º—ã–µ —Ñ–∞–π–ª—ã

```
lg/stats/
‚îú‚îÄ‚îÄ load.py          ‚ùå –£–î–ê–õ–ò–¢–¨ –ü–û–õ–ù–û–°–¢–¨–Æ
‚îú‚îÄ‚îÄ model.py         ‚ùå –£–î–ê–õ–ò–¢–¨ –ü–û–õ–ù–û–°–¢–¨–Æ
```

#### –ù–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞

```
lg/stats/
‚îú‚îÄ‚îÄ __init__.py                      ‚öôÔ∏è –û–±–Ω–æ–≤–∏—Ç—å —ç–∫—Å–ø–æ—Ä—Ç—ã
‚îú‚îÄ‚îÄ collector.py                     ‚úÖ –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
‚îú‚îÄ‚îÄ report_builder.py                üìù –£–ø—Ä–æ—Å—Ç–∏—Ç—å (—É–±—Ä–∞—Ç—å –ª–æ–≥–∏–∫—É –ø–ª–∞–Ω–æ–≤)
‚îú‚îÄ‚îÄ report_schema.py                 üìù –û–±–Ω–æ–≤–∏—Ç—å Pydantic-–º–æ–¥–µ–ª–∏
‚îú‚îÄ‚îÄ tokenizers/                      ‚≠ê –ù–û–í–´–ô –ü–û–î–ú–û–î–£–õ–¨
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                  - –§–∞–±—Ä–∏–∫–∞ –∏ –ø—É–±–ª–∏—á–Ω—ã–π API
‚îÇ   ‚îú‚îÄ‚îÄ base.py                      - –ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –∫–ª–∞—Å—Å BaseTokenizer
‚îÇ   ‚îú‚îÄ‚îÄ tiktoken_adapter.py          - –ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è tiktoken
‚îÇ   ‚îú‚îÄ‚îÄ hf_adapter.py                - –ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è HuggingFace tokenizers
‚îÇ   ‚îú‚îÄ‚îÄ sp_adapter.py                - –ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è SentencePiece
‚îÇ   ‚îî‚îÄ‚îÄ model_cache.py               - –ú–µ–Ω–µ–¥–∂–µ—Ä –∫–µ—à–∞ –º–æ–¥–µ–ª–µ–π
```

### 3.2. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```toml
# pyproject.toml

[project]
dependencies = [
    "ruamel.yaml>=0.18",
    "pathspec>=0.12",
    "tiktoken>=0.6",
    "tokenizers>=0.15",           # ‚≠ê –ù–û–í–û–ï
    "sentencepiece>=0.2",         # ‚≠ê –ù–û–í–û–ï
    "huggingface-hub>=0.20",      # ‚≠ê –ù–û–í–û–ï (–¥–ª—è –∞–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–∏)
    "pydantic>=2.0,<3.0",
    "tree-sitter>=0.21",
    # ...
]
```

### 3.3. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ CLI

```python
# lg/cli.py

# –£–î–ê–õ–ò–¢–¨ –æ–ø—Ü–∏—é --model –∏–∑ –≤—Å–µ—Ö –∫–æ–º–∞–Ω–¥
# –î–û–ë–ê–í–ò–¢–¨ –Ω–æ–≤—ã–µ –æ–ø—Ü–∏–∏ –¥–ª—è render/report:
#   --lib <tiktoken|tokenizers|sentencepiece>
#   --encoder <encoder_name>
#   --ctx-limit <int>

# –£–î–ê–õ–ò–¢–¨ –ø–æ–¥–∫–æ–º–∞–Ω–¥—É lg list models

# –î–û–ë–ê–í–ò–¢–¨ –Ω–æ–≤—ã–µ –ø–æ–¥–∫–æ–º–∞–Ω–¥—ã:
#   lg list tokenizer-libs
#   lg list encoders --lib <lib_name>
```

---

## 4. –ò—Ç–æ–≥–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (–¥–∏–∞–≥—Ä–∞–º–º–∞)

```
CLI (lg/cli.py)
  ‚îî‚îÄ> parse args: --lib, --encoder, --ctx-limit
       ‚îî‚îÄ> RunOptions
            ‚îî‚îÄ> RunContext
                 ‚îî‚îÄ> TokenService
                      ‚îî‚îÄ> create_tokenizer(lib, encoder, ctx_limit, root)
                           ‚îú‚îÄ> TiktokenAdapter (–≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ encodings)
                           ‚îú‚îÄ> HFAdapter (HF Hub + ModelCache)
                           ‚îî‚îÄ> SPAdapter (HF Hub + ModelCache + –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã)

ModelCache (lg/stats/tokenizers/model_cache.py)
  ‚îî‚îÄ> lg-cfg/tokenizer-models/
       ‚îú‚îÄ> tokenizers/
       ‚îÇ    ‚îú‚îÄ> gpt2/
       ‚îÇ    ‚îú‚îÄ> bert-base-uncased/
       ‚îÇ    ‚îî‚îÄ> google--gemma-tokenizer/
       ‚îî‚îÄ> sentencepiece/
            ‚îú‚îÄ> google--gemma-2-2b/
            ‚îî‚îÄ> meta-llama--Llama-2-7b-hf/

StatsCollector
  ‚îî‚îÄ> count_text_cached(text)
       ‚îî‚îÄ> TokenService.count_text_cached()
            ‚îî‚îÄ> BaseTokenizer.count_tokens()

RunResult (protocol 5)
  ‚îú‚îÄ tokenizerLib: "tiktoken" | "tokenizers" | "sentencepiece"
  ‚îú‚îÄ encoder: "cl100k_base" | "gpt2" | "google/gemma-2-2b"
  ‚îî‚îÄ ctxLimit: 128000
```
